{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization\n",
    "\n",
    "## –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ\n",
    "\n",
    "–ó–∞–ø—É—Å—Ç–∏—Ç–µ —è—á–µ–π–∫–∏ –ø–æ –ø–æ—Ä—è–¥–∫—É."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def find_project_root(start: Path) -> Path:\n",
    "    \"\"\"–ò—â–µ–º –∫–æ—Ä–µ–Ω—å –ø—Ä–æ–µ–∫—Ç–∞, —á—Ç–æ–±—ã –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ –ø—É—Ç–∏ —Ä–∞–±–æ—Ç–∞–ª–∏ –∏ –≤ Jupyter, –∏ –≤ CLI.\"\"\"\n",
    "    for p in [start] + list(start.parents):\n",
    "        if (p / \"run_pipeline.py\").exists() and (p / \"files\").exists():\n",
    "            return p\n",
    "    return start\n",
    "\n",
    "\n",
    "PROJECT_ROOT = find_project_root(Path.cwd())\n",
    "FILES_DIR = PROJECT_ROOT / \"files\"\n",
    "\n",
    "# –ü–æ–ø—ã—Ç–∫–∞ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å sklearn\n",
    "try:\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer\n",
    "    from sklearn.decomposition import TruncatedSVD\n",
    "    import joblib\n",
    "    HAS_SKLEARN = True\n",
    "except ImportError:\n",
    "    HAS_SKLEARN = False\n",
    "    print(\"–û–®–ò–ë–ö–ê: sklearn –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω!\")\n",
    "    print(\"–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install scikit-learn\")\n",
    "    sys.exit(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ù–ê–°–¢–†–û–ô–ö–ò\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –±–µ—Ä—ë–º —Ñ–∞–π–ª—ã –∏–∑ files/, –Ω–æ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º –∏ –∑–∞–ø—É—Å–∫ –∏–∑ –¥—Ä—É–≥–∏—Ö –ø–∞–ø–æ–∫\n",
    "_default_input = FILES_DIR / \"dataset_cleaned.csv\"\n",
    "if not _default_input.exists():\n",
    "    _default_input = PROJECT_ROOT / \"dataset_cleaned.csv\"\n",
    "\n",
    "INPUT_FILE = str(_default_input)  # –§–∞–π–ª —Å –æ—á–∏—â–µ–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏\n",
    "TEXT_COLUMN = \"cleaned\"  # –°—Ç–æ–ª–±–µ—Ü —Å —Ç–µ–∫—Å—Ç–æ–º –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏\n",
    "\n",
    "# –ö—É–¥–∞ —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: files/vectorized)\n",
    "OUTPUT_DIR = str((FILES_DIR / \"vectorized\") if FILES_DIR.exists() else (PROJECT_ROOT / \"vectorized\"))\n",
    "\n",
    "# –ú–µ—Ç–æ–¥—ã –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏\n",
    "VECTORIZATION_METHODS = {\n",
    "    'tfidf': {\n",
    "        'name': 'TF-IDF',\n",
    "        'class': TfidfVectorizer,\n",
    "        'params': {\n",
    "            'max_features': 5000,  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
    "            'ngram_range': (1, 2),  # –£–Ω–∏–≥—Ä–∞–º–º—ã –∏ –±–∏–≥—Ä–∞–º–º—ã\n",
    "            'min_df': 2,  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —á–∞—Å—Ç–æ—Ç–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞\n",
    "            'max_df': 0.95,  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —á–∞—Å—Ç–æ—Ç–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞\n",
    "            'lowercase': True,\n",
    "            'analyzer': 'word'\n",
    "        }\n",
    "    },\n",
    "    'count': {\n",
    "        'name': 'Count Vectorizer (Bag of Words)',\n",
    "        'class': CountVectorizer,\n",
    "        'params': {\n",
    "            'max_features': 5000,\n",
    "            'ngram_range': (1, 2),\n",
    "            'min_df': 2,\n",
    "            'max_df': 0.95,\n",
    "            'lowercase': True,\n",
    "            'analyzer': 'word'\n",
    "        }\n",
    "    },\n",
    "    'hash': {\n",
    "        'name': 'Hashing Vectorizer',\n",
    "        'class': HashingVectorizer,\n",
    "        'params': {\n",
    "            'n_features': 5000,  # –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä–∞\n",
    "            'ngram_range': (1, 2),\n",
    "            'lowercase': True,\n",
    "            'analyzer': 'word'\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –§–£–ù–ö–¶–ò–ò\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path, text_column, sample_size=None):\n",
    "    \"\"\"–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ CSV —Ñ–∞–π–ª–∞\"\"\"\n",
    "    print(f\"\\nüìÇ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ {file_path}...\")\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"–§–∞–π–ª {file_path} –Ω–µ –Ω–∞–π–¥–µ–Ω!\")\n",
    "    \n",
    "    if sample_size:\n",
    "        df = pd.read_csv(file_path, nrows=sample_size)\n",
    "        print(f\"   –ó–∞–≥—Ä—É–∂–µ–Ω–∞ –≤—ã–±–æ—Ä–∫–∞: {len(df):,} —Å—Ç—Ä–æ–∫\")\n",
    "    else:\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"   –ó–∞–≥—Ä—É–∂–µ–Ω–æ: {len(df):,} —Å—Ç—Ä–æ–∫\")\n",
    "    \n",
    "    if text_column not in df.columns:\n",
    "        raise ValueError(f\"–°—Ç–æ–ª–±–µ—Ü '{text_column}' –Ω–µ –Ω–∞–π–¥–µ–Ω! –î–æ—Å—Ç—É–ø–Ω—ã–µ —Å—Ç–æ–ª–±—Ü—ã: {list(df.columns)}\")\n",
    "    \n",
    "    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—É—Å—Ç—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è\n",
    "    null_count = df[text_column].isna().sum()\n",
    "    if null_count > 0:\n",
    "        print(f\"   ‚ö†Ô∏è  –ù–∞–π–¥–µ–Ω–æ {null_count} –ø—É—Å—Ç—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π, –æ–Ω–∏ –±—É–¥—É—Ç —É–¥–∞–ª–µ–Ω—ã\")\n",
    "        df = df.dropna(subset=[text_column])\n",
    "    \n",
    "    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤\n",
    "    df = df[df[text_column].astype(str).str.len() > 2]\n",
    "    \n",
    "    print(f\"   ‚úÖ –ì–æ—Ç–æ–≤–æ –∫ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏: {len(df):,} —Å—Ç—Ä–æ–∫\")\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text(df, text_column, method='tfidf', output_dir='vectorized'):\n",
    "    \"\"\"\n",
    "    –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ –≤—ã–±—Ä–∞–Ω–Ω—ã–º –º–µ—Ç–æ–¥–æ–º\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : DataFrame\n",
    "        –î–∞—Ç–∞—Ñ—Ä–µ–π–º —Å —Ç–µ–∫—Å—Ç–∞–º–∏\n",
    "    text_column : str\n",
    "        –ù–∞–∑–≤–∞–Ω–∏–µ —Å—Ç–æ–ª–±—Ü–∞ —Å —Ç–µ–∫—Å—Ç–æ–º\n",
    "    method : str\n",
    "        –ú–µ—Ç–æ–¥ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ ('tfidf', 'count', 'hash')\n",
    "    output_dir : str\n",
    "        –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
    "    \"\"\"\n",
    "    if method not in VECTORIZATION_METHODS:\n",
    "        raise ValueError(f\"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –º–µ—Ç–æ–¥: {method}. –î–æ—Å—Ç—É–ø–Ω—ã–µ: {list(VECTORIZATION_METHODS.keys())}\")\n",
    "    \n",
    "    method_info = VECTORIZATION_METHODS[method]\n",
    "    print(f\"\\nüî¢ –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –º–µ—Ç–æ–¥–æ–º: {method_info['name']}\")\n",
    "    \n",
    "    # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
    "    Path(output_dir).mkdir(exist_ok=True)\n",
    "    \n",
    "    # –ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤\n",
    "    texts = df[text_column].fillna(\"\").astype(str).tolist()\n",
    "    print(f\"   –û–±—Ä–∞–±–æ—Ç–∫–∞ {len(texts):,} —Ç–µ–∫—Å—Ç–æ–≤...\")\n",
    "    \n",
    "    # –°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞\n",
    "    vectorizer_class = method_info['class']\n",
    "    params = method_info['params'].copy()\n",
    "    \n",
    "    # –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è HashingVectorizer\n",
    "    if method == 'hash':\n",
    "        vectorizer = vectorizer_class(**params)\n",
    "    else:\n",
    "        vectorizer = vectorizer_class(**params)\n",
    "    \n",
    "    # –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è\n",
    "    print(\"   –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏...\")\n",
    "    vectors = vectorizer.fit_transform(texts)\n",
    "    \n",
    "    print(f\"   ‚úÖ –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞!\")\n",
    "    print(f\"   –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä–æ–≤: {vectors.shape}\")\n",
    "    print(f\"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {vectors.shape[1]:,}\")\n",
    "    \n",
    "    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞\n",
    "    vectorizer_path = os.path.join(output_dir, f'vectorizer_{method}.pkl')\n",
    "    joblib.dump(vectorizer, vectorizer_path)\n",
    "    print(f\"   üíæ –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {vectorizer_path}\")\n",
    "    \n",
    "    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–æ–≤ –≤ —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–∞—Ö\n",
    "    base_name = f'vectors_{method}'\n",
    "    \n",
    "    # 1. Sparse matrix (scipy sparse format) - —Å–∞–º—ã–π —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –¥–ª—è –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "    sparse_path = os.path.join(output_dir, f'{base_name}_sparse.npz')\n",
    "    np.savez_compressed(sparse_path, data=vectors.data, indices=vectors.indices, \n",
    "                       indptr=vectors.indptr, shape=vectors.shape)\n",
    "    print(f\"   üíæ Sparse –º–∞—Ç—Ä–∏—Ü–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {sparse_path}\")\n",
    "    \n",
    "    # 2. Dense matrix (numpy array) - –¥–ª—è –Ω–µ–±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö –∏–ª–∏ –∫–æ–≥–¥–∞ –Ω—É–∂–µ–Ω –ø–æ–ª–Ω—ã–π –¥–æ—Å—Ç—É–ø\n",
    "    if vectors.shape[1] <= 10000:  # –°–æ—Ö—Ä–∞–Ω—è–µ–º dense —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –Ω–µ —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ\n",
    "        dense_vectors = vectors.toarray()\n",
    "        dense_path = os.path.join(output_dir, f'{base_name}_dense.npy')\n",
    "        np.save(dense_path, dense_vectors)\n",
    "        print(f\"   üíæ Dense –º–∞—Ç—Ä–∏—Ü–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {dense_path}\")\n",
    "        \n",
    "        # 3. CSV —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ (–ø–µ—Ä–≤—ã–µ N –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –ø—Ä–æ—Å–º–æ—Ç—Ä–∞)\n",
    "        if vectors.shape[1] <= 1000:\n",
    "            df_vectors = pd.DataFrame(dense_vectors[:, :min(100, vectors.shape[1])])\n",
    "            df_vectors.columns = [f'feature_{i}' for i in df_vectors.columns]\n",
    "            # –î–æ–±–∞–≤–ª—è–µ–º –∏—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ\n",
    "            df_vectors['id'] = df['id'].values if 'id' in df.columns else range(len(df_vectors))\n",
    "            df_vectors['text'] = df[text_column].values\n",
    "            csv_path = os.path.join(output_dir, f'{base_name}_sample.csv')\n",
    "            df_vectors.to_csv(csv_path, index=False, encoding='utf-8')\n",
    "            print(f\"   üíæ –ü—Ä–∏–º–µ—Ä –≤–µ–∫—Ç–æ—Ä–æ–≤ (CSV): {csv_path}\")\n",
    "    \n",
    "    # 4. –°–æ–∑–¥–∞–Ω–∏–µ DataFrame —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ –∏ —Å—Å—ã–ª–∫–æ–π –Ω–∞ –≤–µ–∫—Ç–æ—Ä—ã\n",
    "    df_result = df.copy()\n",
    "    df_result['vector_file'] = f'{base_name}_sparse.npz'\n",
    "    df_result['vector_index'] = range(len(df_result))\n",
    "    metadata_path = os.path.join(output_dir, f'metadata_{method}.csv')\n",
    "    df_result.to_csv(metadata_path, index=False, encoding='utf-8')\n",
    "    print(f\"   üíæ –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {metadata_path}\")\n",
    "    \n",
    "    # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö (–¥–ª—è TF-IDF –∏ Count)\n",
    "    if method in ['tfidf', 'count']:\n",
    "        feature_names = vectorizer.get_feature_names_out()\n",
    "        feature_df = pd.DataFrame({\n",
    "            'feature_index': range(len(feature_names)),\n",
    "            'feature_name': feature_names\n",
    "        })\n",
    "        features_path = os.path.join(output_dir, f'features_{method}.csv')\n",
    "        feature_df.to_csv(features_path, index=False, encoding='utf-8')\n",
    "        print(f\"   üíæ –ù–∞–∑–≤–∞–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {features_path}\")\n",
    "    \n",
    "    return vectors, vectorizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_dimensions(vectors, n_components=100, method='svd'):\n",
    "    \"\"\"\n",
    "    –£–º–µ–Ω—å—à–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –≤–µ–∫—Ç–æ—Ä–æ–≤\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    vectors : sparse matrix\n",
    "        –í–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã\n",
    "    n_components : int\n",
    "        –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏\n",
    "    method : str\n",
    "        –ú–µ—Ç–æ–¥ ('svd' - TruncatedSVD)\n",
    "    \"\"\"\n",
    "    print(f\"\\nüìâ –£–º–µ–Ω—å—à–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –¥–æ {n_components} –∫–æ–º–ø–æ–Ω–µ–Ω—Ç...\")\n",
    "    \n",
    "    if method == 'svd':\n",
    "        reducer = TruncatedSVD(n_components=n_components, random_state=42)\n",
    "        vectors_reduced = reducer.fit_transform(vectors)\n",
    "        print(f\"   ‚úÖ –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —É–º–µ–Ω—å—à–µ–Ω–∞: {vectors.shape} ‚Üí {vectors_reduced.shape}\")\n",
    "        return vectors_reduced, reducer\n",
    "    else:\n",
    "        raise ValueError(f\"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –º–µ—Ç–æ–¥ —É–º–µ–Ω—å—à–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏: {method}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vectors(vector_file, metadata_file=None):\n",
    "    \"\"\"\n",
    "    –ó–∞–≥—Ä—É–∑–∫–∞ —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    vector_file : str\n",
    "        –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å –≤–µ–∫—Ç–æ—Ä–∞–º–∏ (.npz –¥–ª—è sparse, .npy –¥–ª—è dense)\n",
    "    metadata_file : str, optional\n",
    "        –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏\n",
    "    \"\"\"\n",
    "    from scipy.sparse import csr_matrix\n",
    "    \n",
    "    print(f\"\\nüìÇ –ó–∞–≥—Ä—É–∑–∫–∞ –≤–µ–∫—Ç–æ—Ä–æ–≤ –∏–∑ {vector_file}...\")\n",
    "    \n",
    "    if vector_file.endswith('.npz'):\n",
    "        # Sparse matrix\n",
    "        loaded = np.load(vector_file, allow_pickle=True)\n",
    "        vectors = csr_matrix((loaded['data'], loaded['indices'], loaded['indptr']), \n",
    "                            shape=loaded['shape'])\n",
    "        print(f\"   ‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ: {vectors.shape}\")\n",
    "    elif vector_file.endswith('.npy'):\n",
    "        # Dense matrix\n",
    "        vectors = np.load(vector_file)\n",
    "        print(f\"   ‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ: {vectors.shape}\")\n",
    "    else:\n",
    "        raise ValueError(f\"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞: {vector_file}\")\n",
    "    \n",
    "    if metadata_file and os.path.exists(metadata_file):\n",
    "        metadata = pd.read_csv(metadata_file)\n",
    "        print(f\"   ‚úÖ –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã: {len(metadata)} —Å—Ç—Ä–æ–∫\")\n",
    "        return vectors, metadata\n",
    "    \n",
    "    return vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_vectorization(method='tfidf', input_file=None, text_column=None, \n",
    "                      sample_size=None, reduce_dim=False, components=100, \n",
    "                      output_dir=None):\n",
    "    \"\"\"\n",
    "    –ó–∞–ø—É—Å–∫ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ (–≤–µ—Ä—Å–∏—è –¥–ª—è Jupyter notebook)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    method : str\n",
    "        –ú–µ—Ç–æ–¥ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ ('tfidf', 'count', 'hash')\n",
    "    input_file : str, optional\n",
    "        –í—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: INPUT_FILE)\n",
    "    text_column : str, optional\n",
    "        –°—Ç–æ–ª–±–µ—Ü —Å —Ç–µ–∫—Å—Ç–æ–º (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: TEXT_COLUMN)\n",
    "    sample_size : int, optional\n",
    "        –†–∞–∑–º–µ—Ä –≤—ã–±–æ—Ä–∫–∏ (None –¥–ª—è –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö)\n",
    "    reduce_dim : bool\n",
    "        –£–º–µ–Ω—å—à–∏—Ç—å —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä–æ–≤\n",
    "    components : int\n",
    "        –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç –ø—Ä–∏ —É–º–µ–Ω—å—à–µ–Ω–∏–∏ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏\n",
    "    output_dir : str\n",
    "        –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
    "    \"\"\"\n",
    "    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é, –µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω—ã\n",
    "    if input_file is None:\n",
    "        input_file = INPUT_FILE\n",
    "    if text_column is None:\n",
    "        text_column = TEXT_COLUMN\n",
    "    if output_dir is None:\n",
    "        output_dir = OUTPUT_DIR\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"–í–ï–ö–¢–û–†–ò–ó–ê–¶–ò–Ø –¢–ï–ö–°–¢–ê\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"–ú–µ—Ç–æ–¥: {VECTORIZATION_METHODS[method]['name']}\")\n",
    "    print(f\"–í—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª: {input_file}\")\n",
    "    print(f\"–°—Ç–æ–ª–±–µ—Ü —Å —Ç–µ–∫—Å—Ç–æ–º: {text_column}\")\n",
    "    if sample_size:\n",
    "        print(f\"–†–∞–∑–º–µ—Ä –≤—ã–±–æ—Ä–∫–∏: {sample_size:,}\")\n",
    "    else:\n",
    "        print(\"–û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö\")\n",
    "    if reduce_dim:\n",
    "        print(f\"–£–º–µ–Ω—å—à–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏: {components} –∫–æ–º–ø–æ–Ω–µ–Ω—Ç\")\n",
    "    print(f\"–í—ã—Ö–æ–¥–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {output_dir}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
    "    try:\n",
    "        df = load_data(input_file, text_column, sample_size=sample_size)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –¥–∞–Ω–Ω—ã—Ö: {e}\")\n",
    "        raise\n",
    "    \n",
    "    # –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è\n",
    "    try:\n",
    "        vectors, vectorizer = vectorize_text(df, text_column, method=method, output_dir=output_dir)\n",
    "        \n",
    "        # –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–µ —É–º–µ–Ω—å—à–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏\n",
    "        if reduce_dim and vectors.shape[1] > components:\n",
    "            vectors_reduced, reducer = reduce_dimensions(vectors, n_components=components)\n",
    "            \n",
    "            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —É–º–µ–Ω—å—à–µ–Ω–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤\n",
    "            reduced_path = os.path.join(output_dir, f'vectors_{method}_reduced_{components}.npy')\n",
    "            np.save(reduced_path, vectors_reduced)\n",
    "            print(f\"   üíæ –£–º–µ–Ω—å—à–µ–Ω–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {reduced_path}\")\n",
    "            \n",
    "            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–¥—É–∫—Ç–æ—Ä–∞\n",
    "            reducer_path = os.path.join(output_dir, f'reducer_{method}_{components}.pkl')\n",
    "            joblib.dump(reducer, reducer_path)\n",
    "            print(f\"   üíæ –†–µ–¥—É–∫—Ç–æ—Ä —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {reducer_path}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"‚úÖ –í–ï–ö–¢–û–†–ò–ó–ê–¶–ò–Ø –ó–ê–í–ï–†–®–ï–ù–ê –£–°–ü–ï–®–ù–û!\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"\\nüìÅ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏: {output_dir}/\")\n",
    "        print(f\"   - –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä: vectorizer_{method}.pkl\")\n",
    "        print(f\"   - –í–µ–∫—Ç–æ—Ä—ã (sparse): vectors_{method}_sparse.npz\")\n",
    "        if vectors.shape[1] <= 10000:\n",
    "            print(f\"   - –í–µ–∫—Ç–æ—Ä—ã (dense): vectors_{method}_dense.npy\")\n",
    "        print(f\"   - –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ: metadata_{method}.csv\")\n",
    "        if method in ['tfidf', 'count']:\n",
    "            print(f\"   - –ü—Ä–∏–∑–Ω–∞–∫–∏: features_{method}.csv\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –∏–∑ –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏\"\"\"\n",
    "    # –ü–∞—Ä—Å–∏–Ω–≥ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤ –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description='–í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è',\n",
    "        formatter_class=argparse.RawDescriptionHelpFormatter,\n",
    "        epilog=\"\"\"\n",
    "–ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:\n",
    "  python vectorization.py --method tfidf\n",
    "  python vectorization.py --method count --sample 10000\n",
    "  python vectorization.py --method tfidf --reduce-dim --components 200\n",
    "  python vectorization.py --input dataset_cleaned.csv --method hash\n",
    "        \"\"\"\n",
    "    )\n",
    "    \n",
    "    parser.add_argument('--method', '-m', \n",
    "                       choices=['tfidf', 'count', 'hash'],\n",
    "                       default='tfidf',\n",
    "                       help='–ú–µ—Ç–æ–¥ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: tfidf)')\n",
    "    \n",
    "    parser.add_argument('--input', '-i',\n",
    "                       default=INPUT_FILE,\n",
    "                       help=f'–í—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª —Å –¥–∞–Ω–Ω—ã–º–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: {INPUT_FILE})')\n",
    "    \n",
    "    parser.add_argument('--text-column', '-t',\n",
    "                       default=TEXT_COLUMN,\n",
    "                       help=f'–°—Ç–æ–ª–±–µ—Ü —Å —Ç–µ–∫—Å—Ç–æ–º (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: {TEXT_COLUMN})')\n",
    "    \n",
    "    parser.add_argument('--sample', '-s',\n",
    "                       type=int,\n",
    "                       default=None,\n",
    "                       help='–†–∞–∑–º–µ—Ä –≤—ã–±–æ—Ä–∫–∏ (None –¥–ª—è –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö)')\n",
    "    \n",
    "    parser.add_argument('--reduce-dim', '-r',\n",
    "                       action='store_true',\n",
    "                       help='–£–º–µ–Ω—å—à–∏—Ç—å —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä–æ–≤')\n",
    "    \n",
    "    parser.add_argument('--components', '-c',\n",
    "                       type=int,\n",
    "                       default=100,\n",
    "                       help='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç –ø—Ä–∏ —É–º–µ–Ω—å—à–µ–Ω–∏–∏ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 100)')\n",
    "    \n",
    "    parser.add_argument('--output-dir', '-o',\n",
    "                       default=OUTPUT_DIR,\n",
    "                       help=f'–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: {OUTPUT_DIR})')\n",
    "    \n",
    "    args, _ = parser.parse_known_args()\n",
    "    \n",
    "    # –í—ã–∑—ã–≤–∞–µ–º —Ñ—É–Ω–∫—Ü–∏—é –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏\n",
    "    run_vectorization(\n",
    "        method=args.method,\n",
    "        input_file=args.input,\n",
    "        text_column=args.text_column,\n",
    "        sample_size=args.sample,\n",
    "        reduce_dim=args.reduce_dim,\n",
    "        components=args.components,\n",
    "        output_dir=args.output_dir\n",
    "    )\n",
    "\n",
    "\n",
    "# –ó–∞–ø—É—Å–∫ main() —Ç–æ–ª—å–∫–æ –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ –∏–∑ –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏ (–Ω–µ –≤ Jupyter)\n",
    "# –í Jupyter –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ñ—É–Ω–∫—Ü–∏—é run_vectorization() –Ω–∞–ø—Ä—è–º—É—é\n",
    "try:\n",
    "    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –º—ã –Ω–µ –≤ Jupyter/IPython\n",
    "    get_ipython()\n",
    "    # –ï—Å–ª–∏ –º—ã –∑–¥–µ—Å—å, –∑–Ω–∞—á–∏—Ç –º—ã –≤ Jupyter - –Ω–µ –∑–∞–ø—É—Å–∫–∞–µ–º main()\n",
    "except NameError:\n",
    "    # –ú—ã –Ω–µ –≤ Jupyter - –º–æ–∂–Ω–æ –∑–∞–ø—É—Å–∫–∞—Ç—å main()\n",
    "    if __name__ == \"__main__\":\n",
    "        main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
