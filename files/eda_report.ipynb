{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eda Report\n",
    "\n",
    "## –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ\n",
    "\n",
    "–ó–∞–ø—É—Å—Ç–∏—Ç–µ —è—á–µ–π–∫–∏ –ø–æ –ø–æ—Ä—è–¥–∫—É."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from collections import Counter\n",
    "import re\n",
    "from datetime import datetime\n",
    "import sys\n",
    "\n",
    "# –ü–æ–ø—ã—Ç–∫–∞ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å sklearn (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)\n",
    "try:\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.cluster import KMeans\n",
    "    import numpy as np\n",
    "    HAS_SKLEARN = True\n",
    "except ImportError:\n",
    "    HAS_SKLEARN = False\n",
    "    print(\"–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: sklearn –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –ù–µ–∫–æ—Ç–æ—Ä—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –∞–Ω–∞–ª–∏–∑–∞ –±—É–¥—É—Ç –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã.\")\n",
    "    print(\"–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install scikit-learn numpy\")\n",
    "\n",
    "# –ù–∞—Å—Ç—Ä–æ–π–∫–∏\n",
    "INPUT_FILE = \"dataset.csv\"\n",
    "\n",
    "# –ü–∞–ø–∫–∞ –¥–ª—è –æ—Ç—á–µ—Ç–æ–≤\n",
    "REPORTS_DIR = \"reports\"\n",
    "os.makedirs(REPORTS_DIR, exist_ok=True)\n",
    "OUTPUT_REPORT = os.path.join(REPORTS_DIR, \"eda_report.txt\")\n",
    "\n",
    "print(\"–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è EDA...\")\n",
    "\n",
    "try:\n",
    "    \n",
    "    df_sample = pd.read_csv(INPUT_FILE, nrows=10000)\n",
    "    print(f\"–ó–∞–≥—Ä—É–∂–µ–Ω–∞ –≤—ã–±–æ—Ä–∫–∞: {len(df_sample)} —Å—Ç—Ä–æ–∫\")\n",
    "    print(f\"–°—Ç–æ–ª–±—Ü—ã: {list(df_sample.columns)}\")\n",
    "    \n",
    "    total_rows = sum(1 for _ in open(INPUT_FILE)) - 1  # -1 –¥–ª—è –∑–∞–≥–æ–ª–æ–≤–∫–∞\n",
    "    print(f\"–í—Å–µ–≥–æ —Å—Ç—Ä–æ–∫ –≤ —Ñ–∞–π–ª–µ: {total_rows:,}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "text_col = 'text' if 'text' in df_sample.columns else df_sample.columns[1] if len(df_sample.columns) > 1 else df_sample.columns[0]\n",
    "datetime_col = 'datetime' if 'datetime' in df_sample.columns else None\n",
    "\n",
    "print(f\"\\n–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Å—Ç–æ–ª–±–µ—Ü: '{text_col}'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_lines = []\n",
    "report_lines.append(\"EDA –û–¢–ß–Å–¢\")\n",
    "report_lines.append(\"\")\n",
    "\n",
    "\n",
    "report_lines.append(\" –ë–ê–ó–û–í–ê–Ø –ò–ù–§–û–†–ú–ê–¶–ò–Ø –û –î–ê–ù–ù–´–•\")\n",
    "report_lines.append(f\"–í—Å–µ–≥–æ —Å—Ç—Ä–æ–∫ –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ: {total_rows:,}\")\n",
    "report_lines.append(f\"–°—Ç–æ–ª–±—Ü—ã: {', '.join(df_sample.columns)}\")\n",
    "report_lines.append(f\"–†–∞–∑–º–µ—Ä –≤—ã–±–æ—Ä–∫–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞: {len(df_sample):,} —Å—Ç—Ä–æ–∫\")\n",
    "report_lines.append(\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_lines.append(\" –ê–ù–ê–õ–ò–ó –¢–ï–ö–°–¢–û–í–û–ì–û –°–¢–û–õ–ë–¶–ê\")\n",
    "null_count = df_sample[text_col].isna().sum()\n",
    "report_lines.append(f\"–ü—É—Å—Ç—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π: {null_count} ({null_count/len(df_sample)*100:.2f}%)\")\n",
    "text_lengths = df_sample[text_col].fillna(\"\").astype(str).str.len()\n",
    "report_lines.append(f\"\\n–î–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–æ–≤:\")\n",
    "report_lines.append(f\"  –ú–∏–Ω–∏–º—É–º: {text_lengths.min()} —Å–∏–º–≤–æ–ª–æ–≤\")\n",
    "report_lines.append(f\"  –ú–∞–∫—Å–∏–º—É–º: {text_lengths.max()} —Å–∏–º–≤–æ–ª–æ–≤\")\n",
    "report_lines.append(f\"  –°—Ä–µ–¥–Ω–µ–µ: {text_lengths.mean():.1f} —Å–∏–º–≤–æ–ª–æ–≤\")\n",
    "report_lines.append(f\"  –ú–µ–¥–∏–∞–Ω–∞: {text_lengths.median():.1f} —Å–∏–º–≤–æ–ª–æ–≤\")\n",
    "report_lines.append(f\"  Q1 (25%): {text_lengths.quantile(0.25):.1f} —Å–∏–º–≤–æ–ª–æ–≤\")\n",
    "report_lines.append(f\"  Q3 (75%): {text_lengths.quantile(0.75):.1f} —Å–∏–º–≤–æ–ª–æ–≤\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = df_sample[text_col].fillna(\"\").astype(str).str.split().str.len()\n",
    "report_lines.append(f\"\\n–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤:\")\n",
    "report_lines.append(f\"  –ú–∏–Ω–∏–º—É–º: {word_counts.min()} —Å–ª–æ–≤\")\n",
    "report_lines.append(f\"  –ú–∞–∫—Å–∏–º—É–º: {word_counts.max()} —Å–ª–æ–≤\")\n",
    "report_lines.append(f\"  –°—Ä–µ–¥–Ω–µ–µ: {word_counts.mean():.1f} —Å–ª–æ–≤\")\n",
    "report_lines.append(f\"  –ú–µ–¥–∏–∞–Ω–∞: {word_counts.median():.1f} —Å–ª–æ–≤\")\n",
    "\n",
    "\n",
    "report_lines.append(\"\\n. –¢–û–ü-20 –°–ê–ú–´–• –ß–ê–°–¢–´–• –§–†–ê–ó\")\n",
    "\n",
    "top_phrases = Counter(df_sample[text_col].fillna(\"\").astype(str).str.lower().str.strip()).most_common(20)\n",
    "for i, (phrase, count) in enumerate(top_phrases, 1):\n",
    "    if phrase.strip():\n",
    "        report_lines.append(f\"  {i:2d}. '{phrase[:60]}...' ({count} —Ä–∞–∑)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–û–ï –û–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ö–ê–¢–ï–ì–û–†–ò–ô\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[–ê–Ω–∞–ª–∏–∑] –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–π...\")\n",
    "\n",
    "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.lower().strip()\n",
    "    # –£–¥–∞–ª—è–µ–º –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è, –æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –±—É–∫–≤—ã –∏ —Ü–∏—Ñ—Ä—ã\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text\n",
    "\n",
    "# –†—É—Å—Å–∫–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ (—á–∞—Å—Ç—ã–µ —Å–ª–æ–≤–∞, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ –Ω–µ—Å—É—Ç —Å–º—ã—Å–ª–æ–≤–æ–π –Ω–∞–≥—Ä—É–∑–∫–∏)\n",
    "STOP_WORDS = {\n",
    "    '–∏', '–≤', '–≤–æ', '–Ω–µ', '—á—Ç–æ', '–æ–Ω', '–Ω–∞', '—è', '—Å', '—Å–æ', '–∫–∞–∫', '–∞', '—Ç–æ', '–≤—Å–µ',\n",
    "    '–æ–Ω–∞', '—Ç–∞–∫', '–µ–≥–æ', '–Ω–æ', '–¥–∞', '—Ç—ã', '–∫', '—É', '–∂–µ', '–≤—ã', '–∑–∞', '–±—ã', '–ø–æ',\n",
    "    '—Ç–æ–ª—å–∫–æ', '–µ–µ', '–º–Ω–µ', '–±—ã–ª–æ', '–≤–æ—Ç', '–æ—Ç', '–º–µ–Ω—è', '–µ—â–µ', '–Ω–µ—Ç', '–æ', '–∏–∑',\n",
    "    '–µ–º—É', '—Ç–µ–ø–µ—Ä—å', '–∫–æ–≥–¥–∞', '–¥–∞–∂–µ', '–Ω—É', '–≤–¥—Ä—É–≥', '–ª–∏', '–µ—Å–ª–∏', '—É–∂–µ', '–∏–ª–∏',\n",
    "    '–Ω–∏', '–±—ã—Ç—å', '–±—ã–ª', '–Ω–µ–≥–æ', '–¥–æ', '–≤–∞—Å', '–Ω–∏–±—É–¥—å', '–æ–ø—è—Ç—å', '—É–∂', '–≤–∞–º', '–≤–µ–¥—å',\n",
    "    '—Ç–∞–º', '–ø–æ—Ç–æ–º', '—Å–µ–±—è', '–Ω–∏—á–µ–≥–æ', '–µ–π', '–º–æ–∂–µ—Ç', '–æ–Ω–∏', '—Ç—É—Ç', '–≥–¥–µ', '–µ—Å—Ç—å',\n",
    "    '–Ω–∞–¥–æ', '–Ω–µ–π', '–¥–ª—è', '–º—ã', '—Ç–µ–±—è', '–∏—Ö', '—á–µ–º', '–±—ã–ª–∞', '—Å–∞–º', '—á—Ç–æ–±', '–±–µ–∑',\n",
    "    '–±—É–¥—Ç–æ', '—á–µ–≥–æ', '—Ä–∞–∑', '—Ç–æ–∂–µ', '—Å–µ–±–µ', '–ø–æ–¥', '–±—É–¥–µ—Ç', '–∂', '—Ç–æ–≥–¥–∞', '–∫—Ç–æ',\n",
    "    '—ç—Ç–æ—Ç', '—Ç–æ–≥–æ', '–ø–æ—Ç–æ–º—É', '—ç—Ç–æ–≥–æ', '–∫–∞–∫–æ–π', '—Å–æ–≤—Å–µ–º', '–Ω–∏–º', '–∑–¥–µ—Å—å', '—ç—Ç–æ–º',\n",
    "    '–æ–¥–∏–Ω', '–ø–æ—á—Ç–∏', '–º–æ–π', '—Ç–µ–º', '—á—Ç–æ–±—ã', '–Ω–µ–µ', '—Å–µ–π—á–∞—Å', '–±—ã–ª–∏', '–∫—É–¥–∞', '–∑–∞—á–µ–º',\n",
    "    '–≤—Å–µ—Ö', '–Ω–∏–∫–æ–≥–¥–∞', '–º–æ–∂–Ω–æ', '–ø—Ä–∏', '–Ω–∞–∫–æ–Ω–µ—Ü', '–¥–≤–∞', '–æ–±', '–¥—Ä—É–≥–æ–π', '—Ö–æ—Ç—å',\n",
    "    '–ø–æ—Å–ª–µ', '–Ω–∞–¥', '–±–æ–ª—å—à–µ', '—Ç–æ—Ç', '—á–µ—Ä–µ–∑', '—ç—Ç–∏', '–Ω–∞—Å', '–ø—Ä–æ', '–≤—Å–µ–≥–æ', '–Ω–∏—Ö',\n",
    "    '–∫–∞–∫–∞—è', '–º–Ω–æ–≥–æ', '—Ä–∞–∑–≤–µ', '—Ç—Ä–∏', '—ç—Ç—É', '–º–æ—è', '–≤–ø—Ä–æ—á–µ–º', '—Ö–æ—Ä–æ—à–æ', '—Å–≤–æ—é',\n",
    "    '—ç—Ç–æ–π', '–ø–µ—Ä–µ–¥', '–∏–Ω–æ–≥–¥–∞', '–ª—É—á—à–µ', '—á—É—Ç—å', '—Ç–æ–º', '–Ω–µ–ª—å–∑—è', '—Ç–∞–∫–æ–π', '–∏–º',\n",
    "    '–±–æ–ª–µ–µ', '–≤—Å–µ–≥–¥–∞', '–∫–æ–Ω–µ—á–Ω–æ', '–≤—Å—é', '–º–µ–∂–¥—É'\n",
    "}\n",
    "\n",
    "# –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤\n",
    "print(\"  [1/5] –û—á–∏—Å—Ç–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤...\")\n",
    "texts_clean = df_sample[text_col].fillna(\"\").astype(str).apply(preprocess_text)\n",
    "texts_clean = texts_clean[texts_clean.str.len() > 2]  # –£–±–∏—Ä–∞–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ\n",
    "\n",
    "# –ê–Ω–∞–ª–∏–∑ —á–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç–∏ —Å–ª–æ–≤\n",
    "print(\"  [2/5] –ê–Ω–∞–ª–∏–∑ —á–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç–∏ —Å–ª–æ–≤...\")\n",
    "all_words = []\n",
    "for text in texts_clean:\n",
    "    words = text.split()\n",
    "    # –§–∏–ª—å—Ç—Ä—É–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ –∏ –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–ª–æ–≤–∞\n",
    "    words = [w for w in words if len(w) > 2 and w not in STOP_WORDS]\n",
    "    all_words.extend(words)\n",
    "\n",
    "word_freq = Counter(all_words)\n",
    "\n",
    "# –ê–Ω–∞–ª–∏–∑ –±–∏–≥—Ä–∞–º–º (–¥–≤—É—Ö—Å–ª–æ–≤–Ω—ã—Ö —Ñ—Ä–∞–∑)\n",
    "print(\"  [3/5] –ê–Ω–∞–ª–∏–∑ —Ñ—Ä–∞–∑ (–±–∏–≥—Ä–∞–º–º—ã –∏ —Ç—Ä–∏–≥—Ä–∞–º–º—ã)...\")\n",
    "bigrams = []\n",
    "trigrams = []\n",
    "for text in texts_clean:\n",
    "    words = [w for w in text.split() if len(w) > 2 and w not in STOP_WORDS]\n",
    "    # –ë–∏–≥—Ä–∞–º–º—ã\n",
    "    for i in range(len(words) - 1):\n",
    "        bigrams.append(f\"{words[i]} {words[i+1]}\")\n",
    "    # –¢—Ä–∏–≥—Ä–∞–º–º—ã\n",
    "    for i in range(len(words) - 2):\n",
    "        trigrams.append(f\"{words[i]} {words[i+1]} {words[i+2]}\")\n",
    "\n",
    "bigram_freq = Counter(bigrams)\n",
    "trigram_freq = Counter(trigrams)\n",
    "\n",
    "# TF-IDF –∞–Ω–∞–ª–∏–∑ –¥–ª—è –≤—ã–¥–µ–ª–µ–Ω–∏—è –∑–Ω–∞—á–∏–º—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤\n",
    "print(\"  [4/5] TF-IDF –∞–Ω–∞–ª–∏–∑ –¥–ª—è –≤—ã–¥–µ–ª–µ–Ω–∏—è –∫–ª—é—á–µ–≤—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤...\")\n",
    "if HAS_SKLEARN:\n",
    "    try:\n",
    "        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 5000 —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è TF-IDF (–¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏)\n",
    "        texts_for_tfidf = texts_clean.head(5000).tolist()\n",
    "        if len(texts_for_tfidf) > 100:\n",
    "            vectorizer = TfidfVectorizer(max_features=100, ngram_range=(1, 2), min_df=2, max_df=0.95)\n",
    "            tfidf_matrix = vectorizer.fit_transform(texts_for_tfidf)\n",
    "            feature_names = vectorizer.get_feature_names_out()\n",
    "            \n",
    "            # –ü–æ–ª—É—á–∞–µ–º —Å—Ä–µ–¥–Ω–∏–µ TF-IDF –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–µ—Ä–º–∏–Ω–∞\n",
    "            mean_tfidf = np.mean(tfidf_matrix.toarray(), axis=0)\n",
    "            top_tfidf_indices = np.argsort(mean_tfidf)[-30:][::-1]  # –¢–æ–ø-30\n",
    "            top_tfidf_terms = [(feature_names[i], mean_tfidf[i]) for i in top_tfidf_indices]\n",
    "        else:\n",
    "            top_tfidf_terms = []\n",
    "    except Exception as e:\n",
    "        print(f\"    –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: TF-IDF –∞–Ω–∞–ª–∏–∑ –Ω–µ –≤—ã–ø–æ–ª–Ω–µ–Ω ({e})\")\n",
    "        top_tfidf_terms = []\n",
    "else:\n",
    "    print(\"    –ü—Ä–æ–ø—É—â–µ–Ω–æ: —Ç—Ä–µ–±—É–µ—Ç—Å—è sklearn\")\n",
    "    top_tfidf_terms = []\n",
    "\n",
    "# –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è –æ—Å–Ω–æ–≤–Ω—ã—Ö —Ç–µ–º\n",
    "print(\"  [5/5] –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è —Ç–µ–º...\")\n",
    "texts_for_cluster = []  # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –æ—Ç—á–µ—Ç–µ\n",
    "if HAS_SKLEARN:\n",
    "    try:\n",
    "        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤—ã–±–æ—Ä–∫—É –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏\n",
    "        texts_for_cluster = texts_clean.head(2000).tolist()\n",
    "        if len(texts_for_cluster) > 50:\n",
    "            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ (–æ—Ç 3 –¥–æ 10)\n",
    "            n_clusters = min(10, max(3, len(texts_for_cluster) // 200))\n",
    "            \n",
    "            cluster_vectorizer = TfidfVectorizer(max_features=50, ngram_range=(1, 2), min_df=2)\n",
    "            cluster_matrix = cluster_vectorizer.fit_transform(texts_for_cluster)\n",
    "            \n",
    "            kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "            clusters = kmeans.fit_predict(cluster_matrix)\n",
    "            \n",
    "            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞\n",
    "            cluster_keywords = {}\n",
    "            feature_names_cluster = cluster_vectorizer.get_feature_names_out()\n",
    "            \n",
    "            for i in range(n_clusters):\n",
    "                cluster_center = kmeans.cluster_centers_[i]\n",
    "                top_indices = np.argsort(cluster_center)[-5:][::-1]\n",
    "                top_words = [feature_names_cluster[idx] for idx in top_indices]\n",
    "                cluster_keywords[i] = {\n",
    "                    'keywords': top_words,\n",
    "                    'size': int(np.sum(clusters == i))\n",
    "                }\n",
    "        else:\n",
    "            cluster_keywords = {}\n",
    "    except Exception as e:\n",
    "        print(f\"    –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –Ω–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∞ ({e})\")\n",
    "        cluster_keywords = {}\n",
    "else:\n",
    "    print(\"    –ü—Ä–æ–ø—É—â–µ–Ω–æ: —Ç—Ä–µ–±—É–µ—Ç—Å—è sklearn\")\n",
    "    cluster_keywords = {}\n",
    "\n",
    "# –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç—á—ë—Ç–∞ –æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è—Ö\n",
    "report_lines.append(\"\\n\" + \"=\"*60)\n",
    "report_lines.append(\" –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–ò –û–ü–†–ï–î–ï–õ–Å–ù–ù–´–ï –ö–ê–¢–ï–ì–û–†–ò–ò\")\n",
    "report_lines.append(\"=\"*60)\n",
    "\n",
    "# –¢–æ–ø –∑–Ω–∞—á–∏–º—ã—Ö —Å–ª–æ–≤\n",
    "report_lines.append(\"\\nüìä –¢–û–ü-30 –°–ê–ú–´–• –ß–ê–°–¢–´–• –ó–ù–ê–ß–ò–ú–´–• –°–õ–û–í:\")\n",
    "top_words = word_freq.most_common(30)\n",
    "for i, (word, count) in enumerate(top_words, 1):\n",
    "    pct = count / len(texts_clean) * 100 if len(texts_clean) > 0 else 0\n",
    "    report_lines.append(f\"  {i:2d}. '{word}': {count} —Ä–∞–∑ ({pct:.1f}% —Ç–µ–∫—Å—Ç–æ–≤)\")\n",
    "\n",
    "# –¢–æ–ø –±–∏–≥—Ä–∞–º–º\n",
    "report_lines.append(\"\\nüìù –¢–û–ü-20 –°–ê–ú–´–• –ß–ê–°–¢–´–• –§–†–ê–ó (2 —Å–ª–æ–≤–∞):\")\n",
    "top_bigrams = bigram_freq.most_common(20)\n",
    "for i, (phrase, count) in enumerate(top_bigrams, 1):\n",
    "    pct = count / len(texts_clean) * 100 if len(texts_clean) > 0 else 0\n",
    "    report_lines.append(f\"  {i:2d}. '{phrase}': {count} —Ä–∞–∑ ({pct:.1f}%)\")\n",
    "\n",
    "# –¢–æ–ø —Ç—Ä–∏–≥—Ä–∞–º–º\n",
    "report_lines.append(\"\\nüìù –¢–û–ü-15 –°–ê–ú–´–• –ß–ê–°–¢–´–• –§–†–ê–ó (3 —Å–ª–æ–≤–∞):\")\n",
    "top_trigrams = trigram_freq.most_common(15)\n",
    "for i, (phrase, count) in enumerate(top_trigrams, 1):\n",
    "    pct = count / len(texts_clean) * 100 if len(texts_clean) > 0 else 0\n",
    "    report_lines.append(f\"  {i:2d}. '{phrase}': {count} —Ä–∞–∑ ({pct:.1f}%)\")\n",
    "\n",
    "# TF-IDF –∫–ª—é—á–µ–≤—ã–µ —Ç–µ—Ä–º–∏–Ω—ã\n",
    "if top_tfidf_terms:\n",
    "    report_lines.append(\"\\nüîë –ö–õ–Æ–ß–ï–í–´–ï –¢–ï–†–ú–ò–ù–´ (TF-IDF –∞–Ω–∞–ª–∏–∑, —Ç–æ–ø-20):\")\n",
    "    for i, (term, score) in enumerate(top_tfidf_terms[:20], 1):\n",
    "        report_lines.append(f\"  {i:2d}. '{term}': {score:.4f}\")\n",
    "\n",
    "# –ö–ª–∞—Å—Ç–µ—Ä—ã (–∫–∞—Ç–µ–≥–æ—Ä–∏–∏)\n",
    "if cluster_keywords:\n",
    "    report_lines.append(\"\\nüéØ –û–°–ù–û–í–ù–´–ï –¢–ï–ú–ê–¢–ò–ß–ï–°–ö–ò–ï –ö–ê–¢–ï–ì–û–†–ò–ò (–∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è):\")\n",
    "    sorted_clusters = sorted(cluster_keywords.items(), key=lambda x: x[1]['size'], reverse=True)\n",
    "    for cluster_id, info in sorted_clusters:\n",
    "        keywords_str = \", \".join(info['keywords'])\n",
    "        pct = info['size'] / len(texts_for_cluster) * 100 if len(texts_for_cluster) > 0 else 0\n",
    "        report_lines.append(f\"  –ö–∞—Ç–µ–≥–æ—Ä–∏—è {cluster_id+1}: {info['size']} —Ç–µ–∫—Å—Ç–æ–≤ ({pct:.1f}%)\")\n",
    "        report_lines.append(f\"    –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: {keywords_str}\")\n",
    "\n",
    "# –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –∞–Ω–∞–ª–∏–∑–∞\n",
    "auto_keywords = set()\n",
    "# –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ–ø-50 —Å–ª–æ–≤\n",
    "auto_keywords.update([w for w, _ in word_freq.most_common(50)])\n",
    "# –î–æ–±–∞–≤–ª—è–µ–º —Å–ª–æ–≤–∞ –∏–∑ —Ç–æ–ø –±–∏–≥—Ä–∞–º–º\n",
    "for phrase, _ in bigram_freq.most_common(30):\n",
    "    auto_keywords.update(phrase.split())\n",
    "\n",
    "report_lines.append(f\"\\nüí° –í—Å–µ–≥–æ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤—ã–¥–µ–ª–µ–Ω–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤: {len(auto_keywords)}\")\n",
    "\n",
    "# –ê–Ω–∞–ª–∏–∑ –ø–æ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã–º –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º\n",
    "report_lines.append(\"\\nüìà –ê–ù–ê–õ–ò–ó –ü–û –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–ò –û–ü–†–ï–î–ï–õ–Å–ù–ù–´–ú –ö–õ–Æ–ß–ï–í–´–ú –°–õ–û–í–ê–ú (—Ç–æ–ø-25):\")\n",
    "keyword_counts_auto = {}\n",
    "for keyword in list(auto_keywords)[:25]:  # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 25 –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞\n",
    "    count = df_sample[text_col].fillna(\"\").astype(str).str.lower().str.contains(\n",
    "        re.escape(keyword), na=False, regex=True\n",
    "    ).sum()\n",
    "    if count > 0:\n",
    "        keyword_counts_auto[keyword] = count\n",
    "\n",
    "sorted_keywords_auto = sorted(keyword_counts_auto.items(), key=lambda x: x[1], reverse=True)\n",
    "for keyword, count in sorted_keywords_auto:\n",
    "    pct = count / len(df_sample) * 100\n",
    "    report_lines.append(f\"  '{keyword}': {count} ({pct:.1f}%)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á—ë—Ç–∞\n",
    "with open(OUTPUT_REPORT, 'w', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(report_lines))\n",
    "\n",
    "print(f\"\\n[OK] EDA –æ—Ç—á—ë—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {OUTPUT_REPORT}\")\n",
    "\n",
    "# –í—ã–≤–æ–¥ –Ω–∞ —ç–∫—Ä–∞–Ω\n",
    "print(\"\\n\" + \"\\n\".join(report_lines))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
