{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n\n",
    "## –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ\n\n–ó–∞–ø—É—Å—Ç–∏—Ç–µ —è—á–µ–π–∫–∏ –ø–æ –ø–æ—Ä—è–¥–∫—É."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "from collections import Counter\n",
    "import sys\n",
    "\n",
    "# –ü–æ–ø—ã—Ç–∫–∞ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å tqdm, –µ—Å–ª–∏ –Ω–µ—Ç - –∏—Å–ø–æ–ª—å–∑—É–µ–º –∑–∞–≥–ª—É—à–∫—É\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "    HAS_TQDM = True\n",
    "except ImportError:\n",
    "    HAS_TQDM = False\n",
    "    # –ó–∞–≥–ª—É—à–∫–∞ –¥–ª—è tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    class tqdm:\n",
    "        def __init__(self, *args, **kwargs):\n",
    "            pass\n",
    "        def __enter__(self):\n",
    "            return self\n",
    "        def __exit__(self, *args):\n",
    "            pass\n",
    "        def update(self, n=1):\n",
    "            pass\n",
    "        @staticmethod\n",
    "        def pandas(*args, **kwargs):\n",
    "            pass\n",
    "\n",
    "# –£–ø—Ä–æ—â—ë–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –±–µ–∑ NLTK –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏\n",
    "\n",
    "# ----------------------------\n",
    "# 1. –ù–∞—Å—Ç—Ä–æ–π–∫–∏\n",
    "# ----------------------------\n",
    "INPUT_FILE = \"dataset.csv\"\n",
    "\n",
    "# –í—ã—Ö–æ–¥–Ω—ã–µ —Ñ–∞–π–ª—ã\n",
    "CLEANED_OUTPUT = \"dataset_cleaned.csv\"\n",
    "REMOVED_OUTPUT = \"dataset_removed.csv\"\n",
    "\n",
    "# –ü–∞–ø–∫–∞ –¥–ª—è –æ—Ç—á–µ—Ç–æ–≤\n",
    "REPORTS_DIR = \"reports\"\n",
    "os.makedirs(REPORTS_DIR, exist_ok=True)\n",
    "REPORT_CLEANED = os.path.join(REPORTS_DIR, \"report_cleaned.txt\")\n",
    "REPORT_REMOVED = os.path.join(REPORTS_DIR, \"report_removed.txt\")\n",
    "\n",
    "# ----------------------------\n",
    "# –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: –¥–æ—Å—Ç–∞–≤–∫–∞ (—Ä–∞—Å—à–∏—Ä–µ–Ω–æ)\n",
    "# ----------------------------\n",
    "DELIVERY_KEYWORDS = {\n",
    "    '–ø–æ—Å—ã–ª–∫', '–¥–æ—Å—Ç–∞–≤–∫', '—Ç—Ä–µ–∫', '–æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–∏', '–ø–æ–ª—É—á', '–∑–∞–±—Ä–∞—Ç—å', '–ø—É–Ω–∫—Ç', '–≤—ã–¥–∞—á',\n",
    "    '—Å—Ç–∞—Ç—É—Å', '–∑–∞–¥–µ—Ä–∂', '–ø–æ—Ç–µ—Ä—è', '–ø–æ–≤—Ä–µ–∂–¥', '–≤–æ–∑–≤—Ä–∞—Ç', '–æ—Ç–ø—Ä–∞–≤', '–∞–¥—Ä–µ—Å', '–∫—É—Ä—å–µ—Ä',\n",
    "    '–ø–æ—á—Ç–∞', '5post', '–ø—è—Ç–µ—Ä–æ—á–∫', '–ø–µ—Ä–µ–∫—Ä–µ—Å—Ç–æ–∫', '–º–∞–≥–∞–∑–∏–Ω', '—è—â–∏–∫', '–ª–æ–∫–µ—Ä',\n",
    "    '–ø—Ä–æ–±–ª–µ–º', '–∂–∞–ª–æ–±', '–æ—à–∏–±–∫', '–∏–∑–º–µ–Ω–∏', '–æ—Ç–º–µ–Ω', '–∑–∞–∫–∞–∑', '—Ç–æ–≤–∞—Ä', '–±—Ä–∞–∫',\n",
    "    '–∂–¥–∞—Ç—å', '–ø—Ä–∏–¥–µ—Ç', '–ø—Ä–∏–¥—ë—Ç', '—Å–∫–æ–ª—å–∫–æ', '–≥–¥–µ', '–ø–æ—á–µ–º—É', '–∫–æ–≥–¥–∞', '–æ—Ç—Å–ª–µ–¥–∏—Ç—å',\n",
    "    # --- –î–û–ë–ê–í–õ–ï–ù–û –¥–ª—è –ø–æ—Å—Ç–∞–º–∞—Ç–æ–≤ ---\n",
    "    '–ø–æ—Å—Ç–∞–º–∞—Ç', '–∞–≤—Ç–æ–º–∞—Ç', '—Ç–µ—Ä–º–∏–Ω–∞–ª', '—è—á–µ–π–∫', '–ª–æ–∫–µ—Ä', '–æ—Ç–∫—Ä—ã', '–ø—É—Å—Ç', '–∏–Ω–≤–µ–Ω—Ç–∞—Ä–∏–∑–∞—Ü', '—Å—Ä–æ–∫ —Ö—Ä–∞–Ω–µ–Ω–∏—è'\n",
    "}\n",
    "\n",
    "# –ó–∞—â–∏—Ç–∞ —á–∞—Å—Ç—ã—Ö –∂–∞–ª–æ–±\n",
    "PROTECTED_PHRASES = {\n",
    "    '–Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç –ø–æ—Å—Ç–∞–º–∞—Ç',\n",
    "    '–Ω–µ –æ—Ç–∫—Ä—ã–ª–∞—Å—å —è—á–µ–π–∫–∞',\n",
    "    '–Ω–µ –æ—Ç–∫—Ä—ã–≤–∞–µ—Ç—Å—è —è—á–µ–π–∫–∞',\n",
    "    '–ø—Ä–æ–¥–ª–∏—Ç—å —Å—Ä–æ–∫ —Ö—Ä–∞–Ω–µ–Ω–∏—è',\n",
    "    '–Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç –∞–≤—Ç–æ–º–∞—Ç',\n",
    "    '–ø—É—Å—Ç–∞—è —è—á–µ–π–∫–∞',\n",
    "    '–∏–Ω–≤–µ–Ω—Ç–∞—Ä–∏–∑–∞—Ü–∏—è –ø–æ—Å—Ç–∞–º–∞—Ç–∞',\n",
    "    '–ø–æ—Å—Ç–∞–º–∞—Ç –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç',\n",
    "    '—è—á–µ–π–∫–∞ –Ω–µ –æ—Ç–∫—Ä—ã–ª–∞—Å—å',\n",
    "    '–Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç —Ç–µ—Ä–º–∏–Ω–∞–ª',\n",
    "    '–∞–≤—Ç–æ–º–∞—Ç –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç',\n",
    "    '—è—á–µ–π–∫–∞ –ø—É—Å—Ç–∞—è'\n",
    "}\n",
    "\n",
    "NON_INFORMATIVE_PHRASES = {\"—Å–ø–∞—Å–∏–±–æ\", \"–æ–∫\", \"–¥–∞\", \"–Ω–µ—Ç\", \"–±–æ–ª—å—à–æ–µ\", \"–±–æ–ª—å—à–æ–µ —Å–ø–∞—Å–∏–±–æ\"}\n",
    "\n",
    "OPERATOR_KEYWORDS = {\n",
    "    '–æ–ø–µ—Ä–∞—Ç–æ—Ä', '–º–µ–Ω–µ–¥–∂–µ—Ä', '–∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç', '–ø–æ–º–æ—â—å', '–ø–æ–¥–¥–µ—Ä–∂–∫–∞', \n",
    "    '—Å–≤—è–∑–∞—Ç—å—Å—è', '–ø–æ–∑–≤–æ–Ω–∏—Ç—å', '–ø–µ—Ä–µ–≤–µ—Å—Ç–∏', '—Å–æ–µ–¥–∏–Ω–∏—Ç—å',\n",
    "    '—Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç', '—á–µ–ª–æ–≤–µ–∫', '—Ä–∞–±–æ—Ç–Ω–∏–∫', '—Å–æ—Ç—Ä—É–¥–Ω–∏–∫', '–∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—è', \n",
    "    '–º–µ—Å—Ç–æ–Ω–∞—Ö–æ–∂–¥–µ–Ω–∏–µ', '—Ç–µ—Ö–ø–æ–¥–¥–µ—Ä–∂–∫–∞', '–º–µ—Å—Ç–æ–Ω–∞—Ö–æ–∂–¥–µ–Ω–∏', '—Ç–µ—Ö–ø–æ–¥–¥–µ—Ä–∂–∫'\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_relevance(text):\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return False, \"empty_or_not_string\"\n",
    "    \n",
    "    text_clean = text.strip().lower()\n",
    "    \n",
    "    # üîí –ó–∞—â–∏—Ç–∞ –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ñ—Ä–∞–∑\n",
    "    if text_clean in PROTECTED_PHRASES:\n",
    "        return True, \"protected_phrase\"\n",
    "    \n",
    "    if len(text_clean) <= 2:\n",
    "        return False, \"too_short\"\n",
    "    \n",
    "    words = text_clean.split()\n",
    "\n",
    "    if text_clean in NON_INFORMATIVE_PHRASES:\n",
    "        return False, \"exact_greeting\"\n",
    "    \n",
    "    if all(word in NON_INFORMATIVE_PHRASES for word in words):\n",
    "        return False, \"all_non_informative\"\n",
    "    \n",
    "    if len(words) <= 2 and all(word in NON_INFORMATIVE_PHRASES for word in words):\n",
    "        return False, \"short_non_informative\"\n",
    "\n",
    "    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤\n",
    "    has_delivery = any(kw in word for word in words for kw in DELIVERY_KEYWORDS)\n",
    "    has_operator = any(kw in text_clean for kw in OPERATOR_KEYWORDS)\n",
    "\n",
    "    if has_delivery and has_operator:\n",
    "        return True, \"delivery_and_operator\"\n",
    "    elif has_delivery:\n",
    "        return True, \"delivery_related\"\n",
    "    elif has_operator:\n",
    "        return True, \"operator_request\"\n",
    "    \n",
    "    if len(words) > 4:\n",
    "        return True, \"long_phrase\"\n",
    "    \n",
    "    return False, \"no_relevant_content\"\n",
    "\n",
    "# ----------------------------\n",
    "# 2. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö (—Å –±–∞—Ç—á-–æ–±—Ä–∞–±–æ—Ç–∫–æ–π –¥–ª—è –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–æ–≤)\n",
    "# ----------------------------\n",
    "if not os.path.exists(INPUT_FILE):\n",
    "    raise FileNotFoundError(f\"–§–∞–π–ª {INPUT_FILE} –Ω–µ –Ω–∞–π–¥–µ–Ω –≤: {os.getcwd()}\")\n",
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏\n",
    "file_size_mb = os.path.getsize(INPUT_FILE) / (1024 * 1024)\n",
    "print(f\"–†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: {file_size_mb:.1f} MB\")\n",
    "\n",
    "# –î–ª—è —Ñ–∞–π–ª–æ–≤ –±–æ–ª—å—à–µ 500MB –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–∞—Ç—á-–æ–±—Ä–∞–±–æ—Ç–∫—É\n",
    "USE_BATCH = file_size_mb > 500\n",
    "BATCH_SIZE = 50000  # —Å—Ç—Ä–æ–∫ –Ω–∞ –±–∞—Ç—á\n",
    "\n",
    "if USE_BATCH:\n",
    "    print(f\"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –±–∞—Ç—á-–æ–±—Ä–∞–±–æ—Ç–∫–∞ (–±–∞—Ç—á = {BATCH_SIZE} —Å—Ç—Ä–æ–∫)\")\n",
    "    # –°–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∂–∞–µ–º –Ω–µ–±–æ–ª—å—à—É—é –≤—ã–±–æ—Ä–∫—É –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã\n",
    "    df_sample = pd.read_csv(INPUT_FILE, nrows=100)\n",
    "    text_col = 'text' if 'text' in df_sample.columns else df_sample.columns[1] if len(df_sample.columns) > 1 else df_sample.columns[0]\n",
    "    print(f\"–û–ø—Ä–µ–¥–µ–ª—ë–Ω —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Å—Ç–æ–ª–±–µ—Ü: '{text_col}'\")\n",
    "    print(f\"–ù–∞—á–∏–Ω–∞–µ–º –±–∞—Ç—á-–æ–±—Ä–∞–±–æ—Ç–∫—É...\")\n",
    "else:\n",
    "    print(\"–ó–∞–≥—Ä—É–∑–∫–∞ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö –≤ –ø–∞–º—è—Ç—å...\")\n",
    "    df = pd.read_csv(INPUT_FILE)\n",
    "    print(f\"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} —Å—Ç—Ä–æ–∫. –°—Ç–æ–ª–±—Ü—ã: {list(df.columns)}\")\n",
    "    text_col = 'text' if 'text' in df.columns else df.columns[1] if len(df.columns) > 1 else df.columns[0]\n",
    "    if text_col not in df.columns:\n",
    "        raise ValueError(f\"–û–∂–∏–¥–∞–µ–º—ã–π —Å—Ç–æ–ª–±–µ—Ü '{text_col}' –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç.\")\n",
    "\n",
    "# ----------------------------\n",
    "# 3. –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞\n",
    "# ----------------------------\n",
    "def basic_clean(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.lower().strip()\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text\n",
    "\n",
    "if USE_BATCH:\n",
    "    # –ë–∞—Ç—á-–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–æ–≤\n",
    "    all_cleaned = []\n",
    "    all_removed = []\n",
    "    total_rows = sum(1 for _ in open(INPUT_FILE)) - 1  # -1 –¥–ª—è –∑–∞–≥–æ–ª–æ–≤–∫–∞\n",
    "    \n",
    "    with tqdm(total=total_rows, desc=\"–û–±—Ä–∞–±–æ—Ç–∫–∞\") as pbar:\n",
    "        for chunk in pd.read_csv(INPUT_FILE, chunksize=BATCH_SIZE):\n",
    "            # –û—á–∏—Å—Ç–∫–∞\n",
    "            chunk['cleaned'] = chunk[text_col].apply(basic_clean)\n",
    "            chunk['cleaned_for_classification'] = chunk['cleaned']\n",
    "            \n",
    "            # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è\n",
    "            results = chunk['cleaned_for_classification'].apply(lambda x: classify_relevance(x))\n",
    "            chunk['is_relevant'] = results.apply(lambda x: x[0])\n",
    "            chunk['removal_reason'] = results.apply(lambda x: x[1])\n",
    "            \n",
    "            # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ\n",
    "            df_cleaned_chunk = chunk[chunk['is_relevant']].copy()\n",
    "            df_removed_chunk = chunk[~chunk['is_relevant']].copy()\n",
    "            \n",
    "            all_cleaned.append(df_cleaned_chunk)\n",
    "            all_removed.append(df_removed_chunk)\n",
    "            \n",
    "            pbar.update(len(chunk))\n",
    "    \n",
    "    # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
    "    print(\"–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...\")\n",
    "    df_cleaned = pd.concat(all_cleaned, ignore_index=True)\n",
    "    df_removed = pd.concat(all_removed, ignore_index=True)\n",
    "    \n",
    "    print(f\"‚úÖ –û—Å—Ç–∞–≤–ª–µ–Ω–æ: {len(df_cleaned)}\")\n",
    "    print(f\"üóëÔ∏è –£–¥–∞–ª–µ–Ω–æ: {len(df_removed)}\")\n",
    "    \n",
    "    # –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤\n",
    "    print(\"–£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤...\")\n",
    "    before_dedup = len(df_cleaned)\n",
    "    df_cleaned = df_cleaned.drop_duplicates(subset=['cleaned'])\n",
    "    after_dedup = len(df_cleaned)\n",
    "    print(f\"–£–¥–∞–ª–µ–Ω–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {before_dedup - after_dedup}\")\n",
    "    \n",
    "    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ\n",
    "    print(\"–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...\")\n",
    "    df_cleaned.to_csv(CLEANED_OUTPUT, index=False, encoding='utf-8')\n",
    "    df_removed.to_csv(REMOVED_OUTPUT, index=False, encoding='utf-8')\n",
    "    \n",
    "else:\n",
    "    # –û–±—ã—á–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è –Ω–µ–±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–æ–≤\n",
    "    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ –±–∞–∑–æ–≤—É—é –æ—á–∏—Å—Ç–∫—É (–±—ã—Å—Ç—Ä–µ–µ –≤ —Ä–∞–∑—ã!)\n",
    "    print(\"–û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞...\")\n",
    "    if HAS_TQDM:\n",
    "        tqdm.pandas(desc=\"–û—á–∏—Å—Ç–∫–∞\")\n",
    "        df['cleaned'] = df[text_col].progress_apply(basic_clean)\n",
    "    else:\n",
    "        df['cleaned'] = df[text_col].apply(basic_clean)\n",
    "    df['cleaned_for_classification'] = df['cleaned']\n",
    "\n",
    "    # ----------------------------\n",
    "    # 4. –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ (—Å –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–º)\n",
    "    # ----------------------------\n",
    "    print(\"–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤...\")\n",
    "    if HAS_TQDM:\n",
    "        tqdm.pandas(desc=\"–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è\")\n",
    "        results = df['cleaned_for_classification'].progress_apply(lambda x: classify_relevance(x))\n",
    "    else:\n",
    "        results = df['cleaned_for_classification'].apply(lambda x: classify_relevance(x))\n",
    "    df['is_relevant'] = results.apply(lambda x: x[0])\n",
    "    df['removal_reason'] = results.apply(lambda x: x[1])\n",
    "\n",
    "    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ\n",
    "    df_cleaned = df[df['is_relevant']].copy()\n",
    "    df_removed = df[~df['is_relevant']].copy()\n",
    "\n",
    "    print(f\"‚úÖ –û—Å—Ç–∞–≤–ª–µ–Ω–æ: {len(df_cleaned)}\")\n",
    "    print(f\"üóëÔ∏è –£–¥–∞–ª–µ–Ω–æ: {len(df_removed)}\")\n",
    "\n",
    "    # ----------------------------\n",
    "    # 5. –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ (—Ç–æ–ª—å–∫–æ –≤ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö)\n",
    "    # ----------------------------\n",
    "    print(\"–£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤...\")\n",
    "    before_dedup = len(df_cleaned)\n",
    "    df_cleaned = df_cleaned.drop_duplicates(subset=['cleaned'])  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—É—é –æ—á–∏—Å—Ç–∫—É\n",
    "    after_dedup = len(df_cleaned)\n",
    "    print(f\"–£–¥–∞–ª–µ–Ω–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {before_dedup - after_dedup}\")\n",
    "\n",
    "    # ----------------------------\n",
    "    # 6. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ\n",
    "    # ----------------------------\n",
    "    df_cleaned.to_csv(CLEANED_OUTPUT, index=False, encoding='utf-8')\n",
    "    df_removed.to_csv(REMOVED_OUTPUT, index=False, encoding='utf-8')\n",
    "\n",
    "# ----------------------------\n",
    "# 7. –û—Ç—á—ë—Ç—ã\n",
    "# ----------------------------\n",
    "# –û—Ç—á—ë—Ç: –æ—á–∏—â–µ–Ω–Ω—ã–µ\n",
    "report_clean = [\n",
    "    \"=== –û–¢–ß–Å–¢: –†–ï–õ–ï–í–ê–ù–¢–ù–´–ï –ó–ê–ü–†–û–°–´ ===\\n\",\n",
    "    f\"–í—Å–µ–≥–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö: {len(df_cleaned)}\",\n",
    "    f\"–ü–æ—Å–ª–µ –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏: {after_dedup}\\n\",\n",
    "    \"–ú–µ—Ç–∫–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç ‚Äî —Ç—Ä–µ–±—É–µ—Ç—Å—è —Ä–∞–∑–º–µ—Ç–∫–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏.\\n\",\n",
    "    \"–£–ü–†–û–©–Å–ù–ù–ê–Ø –û–ë–†–ê–ë–û–¢–ö–ê:\",\n",
    "    \"‚úì –ë–∞–∑–æ–≤–∞—è –æ—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ (–±—ã—Å—Ç—Ä–æ)\",\n",
    "    \"‚úì –£–¥–∞–ª–µ–Ω–∏–µ –∑–Ω–∞–∫–æ–≤ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è\",\n",
    "    \"‚úì –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–±–µ–ª–æ–≤\",\n",
    "    \"‚úì –ö–æ–ª–æ–Ω–∫–∞ 'cleaned' —Å–æ–¥–µ—Ä–∂–∏—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç\"\n",
    "]\n",
    "\n",
    "# –û—Ç—á—ë—Ç: —É–¥–∞–ª—ë–Ω–Ω—ã–µ\n",
    "reason_counts = Counter(df_removed['removal_reason'])\n",
    "top_phrases = Counter(df_removed[text_col].fillna(\"\").astype(str)).most_common(20)\n",
    "\n",
    "report_removed = [\"=== –û–¢–ß–Å–¢: –£–î–ê–õ–Å–ù–ù–´–ï –ó–ê–ü–†–û–°–´ ===\\n\", f\"–í—Å–µ–≥–æ —É–¥–∞–ª–µ–Ω–æ: {len(df_removed)}\\n\"]\n",
    "report_removed.append(\"–ü—Ä–∏—á–∏–Ω—ã —É–¥–∞–ª–µ–Ω–∏—è:\")\n",
    "for reason, cnt in reason_counts.most_common():\n",
    "    report_removed.append(f\"  {reason}: {cnt}\")\n",
    "\n",
    "report_removed.append(\"\\n–¢–æ–ø-20 —É–¥–∞–ª—ë–Ω–Ω—ã—Ö —Ñ—Ä–∞–∑:\")\n",
    "for phrase, cnt in top_phrases:\n",
    "    if phrase.strip():\n",
    "        report_removed.append(f\"  '{phrase}' ‚Äî {cnt}\")\n",
    "\n",
    "# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á—ë—Ç–æ–≤\n",
    "with open(REPORT_CLEANED, 'w', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(report_clean))\n",
    "\n",
    "with open(REPORT_REMOVED, 'w', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(report_removed))\n",
    "\n",
    "print(f\"\\n‚úÖ –ì–æ—Ç–æ–≤–æ!\")\n",
    "print(f\"–†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ ‚Üí {CLEANED_OUTPUT}\")\n",
    "print(f\"–£–¥–∞–ª—ë–Ω–Ω—ã–µ ‚Üí {REMOVED_OUTPUT}\")\n",
    "print(f\"–û—Ç—á—ë—Ç—ã ‚Üí {REPORT_CLEANED}, {REPORT_REMOVED}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}