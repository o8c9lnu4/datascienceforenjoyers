{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization\n\n",
    "## Использование\n\nЗапустите ячейки по порядку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Визуализация векторизованных данных\n",
    "Создает графики и отчеты для анализа векторизованных текстов\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "# Настройка кодировки для Windows\n",
    "if sys.platform == 'win32':\n",
    "    try:\n",
    "        # Пытаемся установить UTF-8 для вывода\n",
    "        if hasattr(sys.stdout, 'reconfigure'):\n",
    "            sys.stdout.reconfigure(encoding='utf-8')\n",
    "        if hasattr(sys.stderr, 'reconfigure'):\n",
    "            sys.stderr.reconfigure(encoding='utf-8')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# Попытка импортировать sklearn\n",
    "try:\n",
    "    from sklearn.decomposition import TruncatedSVD, PCA\n",
    "    from sklearn.manifold import TSNE\n",
    "    import joblib\n",
    "    HAS_SKLEARN = True\n",
    "except ImportError:\n",
    "    HAS_SKLEARN = False\n",
    "    print(\"ОШИБКА: sklearn не установлен!\")\n",
    "    print(\"Установите: pip install scikit-learn\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Попытка импортировать библиотеки визуализации\n",
    "try:\n",
    "    import matplotlib\n",
    "    matplotlib.use('Agg')  # Используем backend без GUI\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    HAS_VIS = True\n",
    "except ImportError:\n",
    "    HAS_VIS = False\n",
    "    print(\"[WARN] Предупреждение: matplotlib/seaborn не установлены!\")\n",
    "    print(\"Визуализации будут пропущены. Установите: pip install matplotlib seaborn\")\n",
    "\n",
    "# Попытка импортировать scipy для работы со sparse матрицами\n",
    "try:\n",
    "    from scipy.sparse import csr_matrix\n",
    "    HAS_SCIPY = True\n",
    "except ImportError:\n",
    "    HAS_SCIPY = False\n",
    "    print(\"[WARN] Предупреждение: scipy не установлен!\")\n",
    "    print(\"Установите: pip install scipy\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# НАСТРОЙКИ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VECTORIZED_DIR = \"vectorized\"  # Директория с векторизованными данными\n",
    "VISUALIZATIONS_DIR = \"visualizations\"  # Директория для сохранения графиков\n",
    "REPORTS_DIR = \"reports\"  # Директория для отчетов\n",
    "\n",
    "# Настройки визуализации\n",
    "DEFAULT_METHOD = \"tfidf\"\n",
    "MAX_SAMPLES_FOR_TSNE = 10000  # Максимальное количество образцов для t-SNE\n",
    "MAX_SAMPLES_FOR_PLOTS = 50000  # Максимальное количество образцов для графиков\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ФУНКЦИИ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vectorized_data(method='tfidf', vectorized_dir='vectorized'):\n",
    "    \"\"\"\n",
    "    Загрузка векторизованных данных\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    method : str\n",
    "        Метод векторизации ('tfidf', 'count', 'hash')\n",
    "    vectorized_dir : str\n",
    "        Директория с векторизованными данными\n",
    "    \"\"\"\n",
    "    print(f\"\\n[LOAD] Загрузка векторизованных данных (метод: {method})...\")\n",
    "    \n",
    "    # Пути к файлам\n",
    "    sparse_path = os.path.join(vectorized_dir, f'vectors_{method}_sparse.npz')\n",
    "    metadata_path = os.path.join(vectorized_dir, f'metadata_{method}.csv')\n",
    "    features_path = os.path.join(vectorized_dir, f'features_{method}.csv')\n",
    "    \n",
    "    # Проверка существования файлов\n",
    "    if not os.path.exists(sparse_path):\n",
    "        raise FileNotFoundError(f\"Файл с векторами не найден: {sparse_path}\")\n",
    "    \n",
    "    # Загрузка sparse матрицы\n",
    "    loaded = np.load(sparse_path, allow_pickle=True)\n",
    "    \n",
    "    # Импорт csr_matrix (локально, чтобы гарантировать доступность)\n",
    "    try:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        from scipy.sparse import csr_matrix\n",
    "    except ImportError:\n",
    "        raise ImportError(\"scipy необходим для загрузки sparse матриц. Установите: pip install scipy\")\n",
    "    \n",
    "    vectors = csr_matrix((loaded['data'], loaded['indices'], loaded['indptr']), \n",
    "                        shape=loaded['shape'])\n",
    "    print(f\"   [OK] Векторы загружены: {vectors.shape}\")\n",
    "    \n",
    "    # Загрузка метаданных\n",
    "    metadata = None\n",
    "    if os.path.exists(metadata_path):\n",
    "        metadata = pd.read_csv(metadata_path)\n",
    "        print(f\"   [OK] Метаданные загружены: {len(metadata)} строк\")\n",
    "    \n",
    "    # Загрузка названий признаков (для TF-IDF и Count)\n",
    "    feature_names = None\n",
    "    if method in ['tfidf', 'count'] and os.path.exists(features_path):\n",
    "        features_df = pd.read_csv(features_path)\n",
    "        feature_names = features_df['feature_name'].values\n",
    "        print(f\"   [OK] Названия признаков загружены: {len(feature_names)} признаков\")\n",
    "    \n",
    "    return vectors, metadata, feature_names\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_vector_statistics(vectors):\n",
    "    \"\"\"Расчет статистики по векторам\"\"\"\n",
    "    print(\"\\n[STATS] Расчет статистики по векторам...\")\n",
    "    \n",
    "    stats = {}\n",
    "    \n",
    "    # Основные характеристики\n",
    "    stats['shape'] = vectors.shape\n",
    "    stats['n_samples'] = vectors.shape[0]\n",
    "    stats['n_features'] = vectors.shape[1]\n",
    "    \n",
    "    # Разреженность\n",
    "    if hasattr(vectors, 'nnz'):  # Sparse matrix\n",
    "        total_elements = vectors.shape[0] * vectors.shape[1]\n",
    "        stats['sparsity'] = 1 - (vectors.nnz / total_elements)\n",
    "        stats['non_zero_elements'] = vectors.nnz\n",
    "        stats['sparsity_percent'] = stats['sparsity'] * 100\n",
    "    else:  # Dense matrix\n",
    "        non_zero = np.count_nonzero(vectors)\n",
    "        total_elements = vectors.size\n",
    "        stats['sparsity'] = 1 - (non_zero / total_elements)\n",
    "        stats['non_zero_elements'] = non_zero\n",
    "        stats['sparsity_percent'] = stats['sparsity'] * 100\n",
    "    \n",
    "    # Статистика по строкам (документам)\n",
    "    if hasattr(vectors, 'getnnz'):\n",
    "        # Для sparse матрицы\n",
    "        row_nnz = vectors.getnnz(axis=1)\n",
    "        stats['mean_features_per_doc'] = np.mean(row_nnz)\n",
    "        stats['median_features_per_doc'] = np.median(row_nnz)\n",
    "        stats['min_features_per_doc'] = np.min(row_nnz)\n",
    "        stats['max_features_per_doc'] = np.max(row_nnz)\n",
    "    else:\n",
    "        # Для dense матрицы\n",
    "        row_nnz = np.count_nonzero(vectors, axis=1)\n",
    "        stats['mean_features_per_doc'] = np.mean(row_nnz)\n",
    "        stats['median_features_per_doc'] = np.median(row_nnz)\n",
    "        stats['min_features_per_doc'] = np.min(row_nnz)\n",
    "        stats['max_features_per_doc'] = np.max(row_nnz)\n",
    "    \n",
    "    # Статистика по столбцам (признакам)\n",
    "    if hasattr(vectors, 'getnnz'):\n",
    "        col_nnz = vectors.getnnz(axis=0)\n",
    "        stats['mean_docs_per_feature'] = np.mean(col_nnz)\n",
    "        stats['median_docs_per_feature'] = np.median(col_nnz)\n",
    "        stats['min_docs_per_feature'] = np.min(col_nnz)\n",
    "        stats['max_docs_per_feature'] = np.max(col_nnz)\n",
    "    else:\n",
    "        col_nnz = np.count_nonzero(vectors, axis=0)\n",
    "        stats['mean_docs_per_feature'] = np.mean(col_nnz)\n",
    "        stats['median_docs_per_feature'] = np.median(col_nnz)\n",
    "        stats['min_docs_per_feature'] = np.min(col_nnz)\n",
    "        stats['max_docs_per_feature'] = np.max(col_nnz)\n",
    "    \n",
    "    # Значения (для dense или после преобразования)\n",
    "    if hasattr(vectors, 'data'):\n",
    "        # Sparse matrix\n",
    "        if len(vectors.data) > 0:\n",
    "            stats['mean_value'] = np.mean(vectors.data)\n",
    "            stats['median_value'] = np.median(vectors.data)\n",
    "            stats['min_value'] = np.min(vectors.data)\n",
    "            stats['max_value'] = np.max(vectors.data)\n",
    "            stats['std_value'] = np.std(vectors.data)\n",
    "    else:\n",
    "        # Dense matrix\n",
    "        non_zero_values = vectors[vectors != 0]\n",
    "        if len(non_zero_values) > 0:\n",
    "            stats['mean_value'] = np.mean(non_zero_values)\n",
    "            stats['median_value'] = np.median(non_zero_values)\n",
    "            stats['min_value'] = np.min(non_zero_values)\n",
    "            stats['max_value'] = np.max(non_zero_values)\n",
    "            stats['std_value'] = np.std(non_zero_values)\n",
    "    \n",
    "    return stats\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_sparsity(vectors, output_dir, method='tfidf'):\n",
    "    \"\"\"Визуализация разреженности матрицы\"\"\"\n",
    "    if not HAS_VIS:\n",
    "        return\n",
    "    \n",
    "    print(\"\\n[VIZ] Создание визуализации разреженности...\")\n",
    "    \n",
    "    # Подсчет ненулевых элементов по строкам и столбцам\n",
    "    if hasattr(vectors, 'getnnz'):\n",
    "        row_nnz = vectors.getnnz(axis=1)\n",
    "        col_nnz = vectors.getnnz(axis=0)\n",
    "    else:\n",
    "        row_nnz = np.count_nonzero(vectors, axis=1)\n",
    "        col_nnz = np.count_nonzero(vectors, axis=0)\n",
    "    \n",
    "    # Ограничение выборки для визуализации\n",
    "    n_samples = min(len(row_nnz), MAX_SAMPLES_FOR_PLOTS)\n",
    "    sample_indices = np.random.choice(len(row_nnz), n_samples, replace=False)\n",
    "    row_nnz_sample = row_nnz[sample_indices]\n",
    "    \n",
    "    n_features = min(len(col_nnz), MAX_SAMPLES_FOR_PLOTS)\n",
    "    feature_indices = np.random.choice(len(col_nnz), n_features, replace=False)\n",
    "    col_nnz_sample = col_nnz[feature_indices]\n",
    "    \n",
    "    # Создание фигуры\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle(f'Визуализация разреженности матрицы ({method.upper()})', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Распределение ненулевых элементов по документам\n",
    "    axes[0, 0].hist(row_nnz_sample, bins=50, edgecolor='black', alpha=0.7)\n",
    "    axes[0, 0].set_xlabel('Количество ненулевых признаков в документе')\n",
    "    axes[0, 0].set_ylabel('Частота')\n",
    "    axes[0, 0].set_title('Распределение признаков по документам')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Распределение ненулевых элементов по признакам\n",
    "    axes[0, 1].hist(col_nnz_sample, bins=50, edgecolor='black', alpha=0.7, color='orange')\n",
    "    axes[0, 1].set_xlabel('Количество документов с признаком')\n",
    "    axes[0, 1].set_ylabel('Частота')\n",
    "    axes[0, 1].set_title('Распределение документов по признакам')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Визуализация матрицы (heatmap небольшой выборки)\n",
    "    sample_size = min(100, vectors.shape[0])\n",
    "    feature_size = min(100, vectors.shape[1])\n",
    "    sample_idx = np.random.choice(vectors.shape[0], sample_size, replace=False)\n",
    "    feature_idx = np.random.choice(vectors.shape[1], feature_size, replace=False)\n",
    "    \n",
    "    if hasattr(vectors, 'toarray'):\n",
    "        sample_matrix = vectors[sample_idx][:, feature_idx].toarray()\n",
    "    else:\n",
    "        sample_matrix = vectors[np.ix_(sample_idx, feature_idx)]\n",
    "    \n",
    "    im = axes[1, 0].imshow(sample_matrix, aspect='auto', cmap='viridis', interpolation='nearest')\n",
    "    axes[1, 0].set_xlabel('Признаки (выборка)')\n",
    "    axes[1, 0].set_ylabel('Документы (выборка)')\n",
    "    axes[1, 0].set_title(f'Визуализация матрицы ({sample_size}x{feature_size})')\n",
    "    plt.colorbar(im, ax=axes[1, 0])\n",
    "    \n",
    "    # 4. Box plot распределения признаков по документам\n",
    "    # Берем несколько документов для примера\n",
    "    n_docs_for_box = min(100, vectors.shape[0])\n",
    "    doc_indices = np.random.choice(vectors.shape[0], n_docs_for_box, replace=False)\n",
    "    \n",
    "    if hasattr(vectors, 'getnnz'):\n",
    "        doc_features = [vectors.getrow(i).getnnz() for i in doc_indices]\n",
    "    else:\n",
    "        doc_features = [np.count_nonzero(vectors[i]) for i in doc_indices]\n",
    "    \n",
    "    axes[1, 1].boxplot(doc_features, vert=True)\n",
    "    axes[1, 1].set_ylabel('Количество ненулевых признаков')\n",
    "    axes[1, 1].set_title('Box plot: признаки в документах')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Сохранение\n",
    "    output_path = os.path.join(output_dir, f'sparsity_{method}.png')\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"   [SAVE] Сохранено: {output_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_top_features(vectors, feature_names, output_dir, method='tfidf', top_n=50):\n",
    "    \"\"\"Визуализация топ признаков\"\"\"\n",
    "    if not HAS_VIS or feature_names is None:\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n[VIZ] Создание визуализации топ-{top_n} признаков...\")\n",
    "    \n",
    "    # Подсчет частоты признаков (сумма по столбцам)\n",
    "    if hasattr(vectors, 'sum'):\n",
    "        feature_sums = np.array(vectors.sum(axis=0)).flatten()\n",
    "    else:\n",
    "        feature_sums = np.sum(vectors, axis=0)\n",
    "    \n",
    "    # Получение топ признаков\n",
    "    top_indices = np.argsort(feature_sums)[-top_n:][::-1]\n",
    "    top_features = feature_names[top_indices]\n",
    "    top_values = feature_sums[top_indices]\n",
    "    \n",
    "    # Создание графика\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "    fig.suptitle(f'Топ-{top_n} признаков ({method.upper()})', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Горизонтальный bar chart\n",
    "    axes[0].barh(range(len(top_features)), top_values, color='steelblue')\n",
    "    axes[0].set_yticks(range(len(top_features)))\n",
    "    axes[0].set_yticklabels(top_features, fontsize=8)\n",
    "    axes[0].set_xlabel('Сумма значений признака')\n",
    "    axes[0].set_title('Топ признаков (по сумме)')\n",
    "    axes[0].invert_yaxis()\n",
    "    axes[0].grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    # 2. Логарифмический график\n",
    "    axes[1].bar(range(len(top_features)), top_values, color='coral')\n",
    "    axes[1].set_xticks(range(len(top_features)))\n",
    "    axes[1].set_xticklabels(top_features, rotation=45, ha='right', fontsize=8)\n",
    "    axes[1].set_ylabel('Сумма значений признака (log scale)')\n",
    "    axes[1].set_title('Топ признаков (логарифмическая шкала)')\n",
    "    axes[1].set_yscale('log')\n",
    "    axes[1].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Сохранение\n",
    "    output_path = os.path.join(output_dir, f'top_features_{method}.png')\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"   [SAVE] Сохранено: {output_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_dimension_reduction(vectors, output_dir, method='tfidf', n_samples=None, n_components=2):\n",
    "    \"\"\"Визуализация после уменьшения размерности (PCA/t-SNE)\"\"\"\n",
    "    if not HAS_VIS:\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n[VIZ] Создание визуализации после уменьшения размерности...\")\n",
    "    \n",
    "    # Определение количества образцов\n",
    "    if n_samples is None:\n",
    "        n_samples = min(vectors.shape[0], MAX_SAMPLES_FOR_TSNE)\n",
    "    \n",
    "    if vectors.shape[0] > n_samples:\n",
    "        print(f\"   Используется выборка: {n_samples} из {vectors.shape[0]} документов\")\n",
    "        sample_indices = np.random.choice(vectors.shape[0], n_samples, replace=False)\n",
    "        vectors_sample = vectors[sample_indices]\n",
    "    else:\n",
    "        vectors_sample = vectors\n",
    "        sample_indices = np.arange(vectors.shape[0])\n",
    "    \n",
    "    # Преобразование в dense, если нужно\n",
    "    if hasattr(vectors_sample, 'toarray'):\n",
    "        print(\"   Преобразование sparse матрицы в dense...\")\n",
    "        vectors_dense = vectors_sample.toarray()\n",
    "    else:\n",
    "        vectors_dense = vectors_sample\n",
    "    \n",
    "    # PCA\n",
    "    print(\"   Выполнение PCA...\")\n",
    "    pca = PCA(n_components=min(50, vectors_dense.shape[1]))\n",
    "    vectors_pca = pca.fit_transform(vectors_dense)\n",
    "    \n",
    "    # Визуализация PCA\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 16))\n",
    "    fig.suptitle(f'Уменьшение размерности ({method.upper()})', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. PCA 2D\n",
    "    axes[0, 0].scatter(vectors_pca[:, 0], vectors_pca[:, 1], alpha=0.5, s=10)\n",
    "    axes[0, 0].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.2f}% variance)')\n",
    "    axes[0, 0].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.2f}% variance)')\n",
    "    axes[0, 0].set_title('PCA визуализация')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Объясненная дисперсия\n",
    "    n_components_to_show = min(50, len(pca.explained_variance_ratio_))\n",
    "    axes[0, 1].plot(range(1, n_components_to_show + 1), \n",
    "                    pca.explained_variance_ratio_[:n_components_to_show], \n",
    "                    marker='o', linewidth=2, markersize=4)\n",
    "    axes[0, 1].set_xlabel('PCA компонент')\n",
    "    axes[0, 1].set_ylabel('мера разброса данных ')\n",
    "    axes[0, 1].set_title('мера разброса данных по компонентам')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Кумулятивная объясненная дисперсия\n",
    "    cumulative_variance = np.cumsum(pca.explained_variance_ratio_[:n_components_to_show])\n",
    "    axes[1, 0].plot(range(1, n_components_to_show + 1), cumulative_variance, \n",
    "                    marker='o', linewidth=2, markersize=4, color='green')\n",
    "    axes[1, 0].axhline(y=0.8, color='r', linestyle='--', label='80% variance')\n",
    "    axes[1, 0].axhline(y=0.9, color='orange', linestyle='--', label='90% variance')\n",
    "    axes[1, 0].set_xlabel('PCA кумуляция ')\n",
    "    axes[1, 0].set_ylabel('общая доля главных компонентов')\n",
    "    axes[1, 0].set_title('общая доля главных компонентов')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. PCA 3D (первые 3 компоненты)\n",
    "    if vectors_pca.shape[1] >= 3:\n",
    "        ax_3d = fig.add_subplot(2, 2, 4, projection='3d')\n",
    "        ax_3d.scatter(vectors_pca[:, 0], vectors_pca[:, 1], vectors_pca[:, 2], \n",
    "                     alpha=0.5, s=10)\n",
    "        ax_3d.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.2f}%)')\n",
    "        ax_3d.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.2f}%)')\n",
    "        ax_3d.set_zlabel(f'PC3 ({pca.explained_variance_ratio_[2]*100:.2f}%)')\n",
    "        ax_3d.set_title('PCA 3d визуализация')\n",
    "    else:\n",
    "        axes[1, 1].text(0.5, 0.5, 'Недостаточно компонент\\nдля 3D визуализации', \n",
    "                        ha='center', va='center', fontsize=12)\n",
    "        axes[1, 1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Сохранение\n",
    "    output_path = os.path.join(output_dir, f'dimension_reduction_{method}.png')\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"   [SAVE] Сохранено: {output_path}\")\n",
    "    \n",
    "    # t-SNE (только для небольших выборок)\n",
    "    if n_samples <= MAX_SAMPLES_FOR_TSNE:\n",
    "        print(\"   Выполнение t-SNE (это может занять время)...\")\n",
    "        try:\n",
    "            tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000)\n",
    "            vectors_tsne = tsne.fit_transform(vectors_dense)\n",
    "            \n",
    "            plt.figure(figsize=(12, 10))\n",
    "            plt.scatter(vectors_tsne[:, 0], vectors_tsne[:, 1], alpha=0.5, s=10)\n",
    "            plt.xlabel('t-SNE компонента 1')\n",
    "            plt.ylabel('t-SNE компонента 2')\n",
    "            plt.title(f't-SNE визуализация ({method.upper()}, {n_samples} образцов)')\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            \n",
    "            output_path = os.path.join(output_dir, f'tsne_{method}.png')\n",
    "            plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            print(f\"   [SAVE] Сохранено: {output_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   [WARN] Ошибка при выполнении t-SNE: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_text_report(stats, output_dir, method='tfidf'):\n",
    "    \"\"\"Создание текстового отчета\"\"\"\n",
    "    print(\"\\n[REPORT] Создание текстового отчета...\")\n",
    "    \n",
    "    report_lines = []\n",
    "    report_lines.append(\"=\"*60)\n",
    "    report_lines.append(f\"ОТЧЕТ ПО ВИЗУАЛИЗАЦИИ ВЕКТОРИЗОВАННЫХ ДАННЫХ\")\n",
    "    report_lines.append(f\"Метод: {method.upper()}\")\n",
    "    report_lines.append(\"=\"*60)\n",
    "    report_lines.append(\"\")\n",
    "    \n",
    "    report_lines.append(\"ОСНОВНЫЕ ХАРАКТЕРИСТИКИ:\")\n",
    "    report_lines.append(f\"  Размерность матрицы: {stats['shape'][0]:,} x {stats['shape'][1]:,}\")\n",
    "    report_lines.append(f\"  Количество документов: {stats['n_samples']:,}\")\n",
    "    report_lines.append(f\"  Количество признаков: {stats['n_features']:,}\")\n",
    "    report_lines.append(\"\")\n",
    "    \n",
    "    report_lines.append(\"РАЗРЕЖЕННОСТЬ:\")\n",
    "    report_lines.append(f\"  Ненулевых элементов: {stats['non_zero_elements']:,}\")\n",
    "    report_lines.append(f\"  Разреженность: {stats['sparsity_percent']:.2f}%\")\n",
    "    report_lines.append(\"\")\n",
    "    \n",
    "    report_lines.append(\"СТАТИСТИКА ПО ДОКУМЕНТАМ:\")\n",
    "    report_lines.append(f\"  Среднее признаков на документ: {stats['mean_features_per_doc']:.2f}\")\n",
    "    report_lines.append(f\"  Медиана признаков на документ: {stats['median_features_per_doc']:.2f}\")\n",
    "    report_lines.append(f\"  Мин. признаков на документ: {stats['min_features_per_doc']}\")\n",
    "    report_lines.append(f\"  Макс. признаков на документ: {stats['max_features_per_doc']}\")\n",
    "    report_lines.append(\"\")\n",
    "    \n",
    "    report_lines.append(\"СТАТИСТИКА ПО ПРИЗНАКАМ:\")\n",
    "    report_lines.append(f\"  Среднее документов на признак: {stats['mean_docs_per_feature']:.2f}\")\n",
    "    report_lines.append(f\"  Медиана документов на признак: {stats['median_docs_per_feature']:.2f}\")\n",
    "    report_lines.append(f\"  Мин. документов на признак: {stats['min_docs_per_feature']}\")\n",
    "    report_lines.append(f\"  Макс. документов на признак: {stats['max_docs_per_feature']}\")\n",
    "    report_lines.append(\"\")\n",
    "    \n",
    "    if 'mean_value' in stats:\n",
    "        report_lines.append(\"СТАТИСТИКА ЗНАЧЕНИЙ:\")\n",
    "        report_lines.append(f\"  Среднее значение: {stats['mean_value']:.4f}\")\n",
    "        report_lines.append(f\"  Медиана значений: {stats['median_value']:.4f}\")\n",
    "        report_lines.append(f\"  Мин. значение: {stats['min_value']:.4f}\")\n",
    "        report_lines.append(f\"  Макс. значение: {stats['max_value']:.4f}\")\n",
    "        report_lines.append(f\"  Стд. отклонение: {stats['std_value']:.4f}\")\n",
    "        report_lines.append(\"\")\n",
    "    \n",
    "    report_lines.append(\"=\"*60)\n",
    "    \n",
    "    # Сохранение отчета\n",
    "    report_path = os.path.join(output_dir, f'vectorization_report_{method}.txt')\n",
    "    with open(report_path, 'w', encoding='utf-8') as f:\n",
    "        f.write('\\n'.join(report_lines))\n",
    "    print(f\"   [SAVE] Сохранено: {report_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ОСНОВНАЯ ПРОГРАММА\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Основная функция\"\"\"\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description='Визуализация векторизованных данных',\n",
    "        formatter_class=argparse.RawDescriptionHelpFormatter,\n",
    "        epilog=\"\"\"\n",
    "Примеры использования:\n",
    "  python visualization.py --method tfidf\n",
    "  python visualization.py --method count --no-tsne\n",
    "  python visualization.py --method tfidf --samples 5000\n",
    "        \"\"\"\n",
    "    )\n",
    "    \n",
    "    parser.add_argument('--method', '-m',\n",
    "                       choices=['tfidf', 'count', 'hash'],\n",
    "                       default=DEFAULT_METHOD,\n",
    "                       help=f'Метод векторизации (по умолчанию: {DEFAULT_METHOD})')\n",
    "    \n",
    "    parser.add_argument('--vectorized-dir', '-v',\n",
    "                       default=VECTORIZED_DIR,\n",
    "                       help=f'Директория с векторизованными данными (по умолчанию: {VECTORIZED_DIR})')\n",
    "    \n",
    "    parser.add_argument('--output-dir', '-o',\n",
    "                       default=VISUALIZATIONS_DIR,\n",
    "                       help=f'Директория для сохранения графиков (по умолчанию: {VISUALIZATIONS_DIR})')\n",
    "    \n",
    "    parser.add_argument('--samples', '-s',\n",
    "                       type=int,\n",
    "                       default=None,\n",
    "                       help='Количество образцов для визуализации (по умолчанию: автоматически)')\n",
    "    \n",
    "    parser.add_argument('--no-tsne', '-t',\n",
    "                       action='store_true',\n",
    "                       help='Пропустить t-SNE визуализацию (может быть медленной)')\n",
    "    \n",
    "    parser.add_argument('--top-features', '-f',\n",
    "                       type=int,\n",
    "                       default=50,\n",
    "                       help='Количество топ признаков для визуализации (по умолчанию: 50)')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"ВИЗУАЛИЗАЦИЯ ВЕКТОРИЗОВАННЫХ ДАННЫХ\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Метод: {args.method.upper()}\")\n",
    "    print(f\"Директория с данными: {args.vectorized_dir}\")\n",
    "    print(f\"Директория для графиков: {args.output_dir}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Создание директорий\n",
    "    Path(args.output_dir).mkdir(exist_ok=True)\n",
    "    Path(REPORTS_DIR).mkdir(exist_ok=True)\n",
    "    \n",
    "    # Загрузка данных\n",
    "    try:\n",
    "        vectors, metadata, feature_names = load_vectorized_data(\n",
    "            method=args.method,\n",
    "            vectorized_dir=args.vectorized_dir\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Ошибка при загрузке данных: {e}\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    # Расчет статистики\n",
    "    stats = calculate_vector_statistics(vectors)\n",
    "    \n",
    "    # Создание визуализаций\n",
    "    if HAS_VIS:\n",
    "        visualize_sparsity(vectors, args.output_dir, method=args.method)\n",
    "        \n",
    "        if feature_names is not None:\n",
    "            visualize_top_features(vectors, feature_names, args.output_dir, \n",
    "                                 method=args.method, top_n=args.top_features)\n",
    "        \n",
    "        visualize_dimension_reduction(vectors, args.output_dir, method=args.method, \n",
    "                                    n_samples=args.samples)\n",
    "    else:\n",
    "        print(\"\\n[WARN] Визуализации пропущены (библиотеки не установлены)\")\n",
    "    \n",
    "    # Создание текстового отчета\n",
    "    create_text_report(stats, REPORTS_DIR, method=args.method)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"[SUCCESS] ВИЗУАЛИЗАЦИЯ ЗАВЕРШЕНА УСПЕШНО!\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\n[DIR] Графики сохранены в: {args.output_dir}/\")\n",
    "    print(f\"[DIR] Отчеты сохранены в: {REPORTS_DIR}/\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}