#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–µ—Ç–æ–¥–æ–≤: TF-IDF, Count Vectorizer, Hashing Vectorizer
"""

import pandas as pd
import numpy as np
import os
import sys
import argparse
from pathlib import Path

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∫–æ–¥–∏—Ä–æ–≤–∫–∏ –¥–ª—è Windows (—á—Ç–æ–±—ã –Ω–µ –ø–∞–¥–∞—Ç—å –Ω–∞ Unicode –≤ –∫–æ–Ω—Å–æ–ª–∏)
if sys.platform == 'win32':
    try:
        if hasattr(sys.stdout, 'reconfigure'):
            sys.stdout.reconfigure(encoding='utf-8', errors='replace')
        if hasattr(sys.stderr, 'reconfigure'):
            sys.stderr.reconfigure(encoding='utf-8', errors='replace')
    except Exception:
        pass

# –ü–æ–ø—ã—Ç–∫–∞ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å sklearn
try:
    from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer
    from sklearn.decomposition import TruncatedSVD
    import joblib
    HAS_SKLEARN = True
except ImportError:
    HAS_SKLEARN = False
    print("–û–®–ò–ë–ö–ê: sklearn –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω!")
    print("–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install scikit-learn")
    sys.exit(1)

# –ù–ê–°–¢–†–û–ô–ö–ò
INPUT_FILE = "dataset_cleaned.csv"  # –§–∞–π–ª —Å –æ—á–∏—â–µ–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
TEXT_COLUMN = "cleaned"  # –°—Ç–æ–ª–±–µ—Ü —Å —Ç–µ–∫—Å—Ç–æ–º –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏

# –ú–µ—Ç–æ–¥—ã –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏
VECTORIZATION_METHODS = {
    'tfidf': {
        'name': 'TF-IDF',
        'class': TfidfVectorizer,
        'params': {
            'max_features': 5000,  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            'ngram_range': (1, 2),  # –£–Ω–∏–≥—Ä–∞–º–º—ã –∏ –±–∏–≥—Ä–∞–º–º—ã
            'min_df': 2,  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —á–∞—Å—Ç–æ—Ç–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            'max_df': 0.95,  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —á–∞—Å—Ç–æ—Ç–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            'lowercase': True,
            'analyzer': 'word'
        }
    },
    'count': {
        'name': 'Count Vectorizer (Bag of Words)',
        'class': CountVectorizer,
        'params': {
            'max_features': 5000,
            'ngram_range': (1, 2),
            'min_df': 2,
            'max_df': 0.95,
            'lowercase': True,
            'analyzer': 'word'
        }
    },
    'hash': {
        'name': 'Hashing Vectorizer',
        'class': HashingVectorizer,
        'params': {
            'n_features': 5000,  # –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä–∞
            'ngram_range': (1, 2),
            'lowercase': True,
            'analyzer': 'word'
        }
    }
}

# –§–£–ù–ö–¶–ò–ò

def load_data(file_path, text_column, sample_size=None):
    """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ CSV —Ñ–∞–π–ª–∞"""
    print(f"\n[LOAD] –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ {file_path}...")
    
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"–§–∞–π–ª {file_path} –Ω–µ –Ω–∞–π–¥–µ–Ω!")
    
    if sample_size:
        df = pd.read_csv(file_path, nrows=sample_size)
        print(f"   –ó–∞–≥—Ä—É–∂–µ–Ω–∞ –≤—ã–±–æ—Ä–∫–∞: {len(df):,} —Å—Ç—Ä–æ–∫")
    else:
        df = pd.read_csv(file_path)
        print(f"   –ó–∞–≥—Ä—É–∂–µ–Ω–æ: {len(df):,} —Å—Ç—Ä–æ–∫")
    
    if text_column not in df.columns:
        raise ValueError(f"–°—Ç–æ–ª–±–µ—Ü '{text_column}' –Ω–µ –Ω–∞–π–¥–µ–Ω! –î–æ—Å—Ç—É–ø–Ω—ã–µ —Å—Ç–æ–ª–±—Ü—ã: {list(df.columns)}")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—É—Å—Ç—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
    null_count = df[text_column].isna().sum()
    if null_count > 0:
        print(f"   [WARN] –ù–∞–π–¥–µ–Ω–æ {null_count} –ø—É—Å—Ç—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π, –æ–Ω–∏ –±—É–¥—É—Ç —É–¥–∞–ª–µ–Ω—ã")
        df = df.dropna(subset=[text_column])
    
    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤
    df = df[df[text_column].astype(str).str.len() > 2]
    
    print(f"   [OK] –ì–æ—Ç–æ–≤–æ –∫ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏: {len(df):,} —Å—Ç—Ä–æ–∫")
    return df


def vectorize_text(df, text_column, method='tfidf', output_dir='vectorized'):
    """
    –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ –≤—ã–±—Ä–∞–Ω–Ω—ã–º –º–µ—Ç–æ–¥–æ–º
    
    Parameters:
    -----------
    df : DataFrame
        –î–∞—Ç–∞—Ñ—Ä–µ–π–º —Å —Ç–µ–∫—Å—Ç–∞–º–∏
    text_column : str
        –ù–∞–∑–≤–∞–Ω–∏–µ —Å—Ç–æ–ª–±—Ü–∞ —Å —Ç–µ–∫—Å—Ç–æ–º
    method : str
        –ú–µ—Ç–æ–¥ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ ('tfidf', 'count', 'hash')
    output_dir : str
        –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    """
    if method not in VECTORIZATION_METHODS:
        raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –º–µ—Ç–æ–¥: {method}. –î–æ—Å—Ç—É–ø–Ω—ã–µ: {list(VECTORIZATION_METHODS.keys())}")
    
    method_info = VECTORIZATION_METHODS[method]
    print(f"\n[VEC] –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –º–µ—Ç–æ–¥–æ–º: {method_info['name']}")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    Path(output_dir).mkdir(exist_ok=True)
    
    # –ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤
    texts = df[text_column].fillna("").astype(str).tolist()
    print(f"   –û–±—Ä–∞–±–æ—Ç–∫–∞ {len(texts):,} —Ç–µ–∫—Å—Ç–æ–≤...")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞
    vectorizer_class = method_info['class']
    params = method_info['params'].copy()
    
    # –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è HashingVectorizer
    if method == 'hash':
        vectorizer = vectorizer_class(**params)
    else:
        vectorizer = vectorizer_class(**params)
    
    # –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è
    print("   –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏...")
    vectors = vectorizer.fit_transform(texts)
    
    print(f"   [OK] –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
    print(f"   –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä–æ–≤: {vectors.shape}")
    print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {vectors.shape[1]:,}")
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞
    vectorizer_path = os.path.join(output_dir, f'vectorizer_{method}.pkl')
    joblib.dump(vectorizer, vectorizer_path)
    print(f"   [SAVE] –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {vectorizer_path}")
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–æ–≤ –≤ —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–∞—Ö
    base_name = f'vectors_{method}'
    
    # 1. Sparse matrix (scipy sparse format) - —Å–∞–º—ã–π —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –¥–ª—è –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö
    sparse_path = os.path.join(output_dir, f'{base_name}_sparse.npz')
    np.savez_compressed(sparse_path, data=vectors.data, indices=vectors.indices, 
                       indptr=vectors.indptr, shape=vectors.shape)
    print(f"   [SAVE] Sparse –º–∞—Ç—Ä–∏—Ü–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {sparse_path}")
    
    # 2. Dense matrix (numpy array) - –¥–ª—è –Ω–µ–±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö –∏–ª–∏ –∫–æ–≥–¥–∞ –Ω—É–∂–µ–Ω –ø–æ–ª–Ω—ã–π –¥–æ—Å—Ç—É–ø
    if vectors.shape[1] <= 10000:  # –°–æ—Ö—Ä–∞–Ω—è–µ–º dense —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –Ω–µ —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ
        dense_vectors = vectors.toarray()
        dense_path = os.path.join(output_dir, f'{base_name}_dense.npy')
        np.save(dense_path, dense_vectors)
        print(f"   [SAVE] Dense –º–∞—Ç—Ä–∏—Ü–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {dense_path}")
        
        # 3. CSV —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ (–ø–µ—Ä–≤—ã–µ N –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –ø—Ä–æ—Å–º–æ—Ç—Ä–∞)
        if vectors.shape[1] <= 1000:
            df_vectors = pd.DataFrame(dense_vectors[:, :min(100, vectors.shape[1])])
            df_vectors.columns = [f'feature_{i}' for i in df_vectors.columns]
            # –î–æ–±–∞–≤–ª—è–µ–º –∏—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            df_vectors['id'] = df['id'].values if 'id' in df.columns else range(len(df_vectors))
            df_vectors['text'] = df[text_column].values
            csv_path = os.path.join(output_dir, f'{base_name}_sample.csv')
            df_vectors.to_csv(csv_path, index=False, encoding='utf-8')
            print(f"   [SAVE] –ü—Ä–∏–º–µ—Ä –≤–µ–∫—Ç–æ—Ä–æ–≤ (CSV): {csv_path}")
    
    # 4. –°–æ–∑–¥–∞–Ω–∏–µ DataFrame —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ –∏ —Å—Å—ã–ª–∫–æ–π –Ω–∞ –≤–µ–∫—Ç–æ—Ä—ã
    df_result = df.copy()
    df_result['vector_file'] = f'{base_name}_sparse.npz'
    df_result['vector_index'] = range(len(df_result))
    metadata_path = os.path.join(output_dir, f'metadata_{method}.csv')
    df_result.to_csv(metadata_path, index=False, encoding='utf-8')
    print(f"   [SAVE] –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {metadata_path}")
    
    # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö (–¥–ª—è TF-IDF –∏ Count)
    if method in ['tfidf', 'count']:
        feature_names = vectorizer.get_feature_names_out()
        feature_df = pd.DataFrame({
            'feature_index': range(len(feature_names)),
            'feature_name': feature_names
        })
        features_path = os.path.join(output_dir, f'features_{method}.csv')
        feature_df.to_csv(features_path, index=False, encoding='utf-8')
        print(f"   [SAVE] –ù–∞–∑–≤–∞–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {features_path}")
    
    return vectors, vectorizer


def reduce_dimensions(vectors, n_components=100, method='svd'):
    """
    –£–º–µ–Ω—å—à–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –≤–µ–∫—Ç–æ—Ä–æ–≤
    
    Parameters:
    -----------
    vectors : sparse matrix
        –í–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã
    n_components : int
        –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
    method : str
        –ú–µ—Ç–æ–¥ ('svd' - TruncatedSVD)
    """
    print(f"\n[REDUCE] –£–º–µ–Ω—å—à–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –¥–æ {n_components} –∫–æ–º–ø–æ–Ω–µ–Ω—Ç...")
    
    if method == 'svd':
        reducer = TruncatedSVD(n_components=n_components, random_state=42)
        vectors_reduced = reducer.fit_transform(vectors)
        print(f"   [OK] –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —É–º–µ–Ω—å—à–µ–Ω–∞: {vectors.shape} -> {vectors_reduced.shape}")
        return vectors_reduced, reducer
    else:
        raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –º–µ—Ç–æ–¥ —É–º–µ–Ω—å—à–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏: {method}")


def load_vectors(vector_file, metadata_file=None):
    """
    –ó–∞–≥—Ä—É–∑–∫–∞ —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤
    
    Parameters:
    -----------
    vector_file : str
        –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å –≤–µ–∫—Ç–æ—Ä–∞–º–∏ (.npz –¥–ª—è sparse, .npy –¥–ª—è dense)
    metadata_file : str, optional
        –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
    """
    print(f"\n[LOAD] –ó–∞–≥—Ä—É–∑–∫–∞ –≤–µ–∫—Ç–æ—Ä–æ–≤ –∏–∑ {vector_file}...")
    
    if vector_file.endswith('.npz'):
        # Sparse matrix
        loaded = np.load(vector_file, allow_pickle=True)
        from scipy.sparse import csr_matrix
        vectors = csr_matrix((loaded['data'], loaded['indices'], loaded['indptr']), 
                            shape=loaded['shape'])
        print(f"   [OK] –ó–∞–≥—Ä—É–∂–µ–Ω–æ: {vectors.shape}")
    elif vector_file.endswith('.npy'):
        # Dense matrix
        vectors = np.load(vector_file)
        print(f"   [OK] –ó–∞–≥—Ä—É–∂–µ–Ω–æ: {vectors.shape}")
    else:
        raise ValueError(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞: {vector_file}")
    
    if metadata_file and os.path.exists(metadata_file):
        metadata = pd.read_csv(metadata_file)
        print(f"   [OK] –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã: {len(metadata)} —Å—Ç—Ä–æ–∫")
        return vectors, metadata
    
    return vectors


# –û–°–ù–û–í–ù–ê–Ø –ü–†–û–ì–†–ê–ú–ú–ê

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    # –ü–∞—Ä—Å–∏–Ω–≥ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤ –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏
    parser = argparse.ArgumentParser(
        description='–í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
–ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
  python vectorization.py --method tfidf
  python vectorization.py --method count --sample 10000
  python vectorization.py --method tfidf --reduce-dim --components 200
  python vectorization.py --input dataset_cleaned.csv --method hash
        """
    )
    
    parser.add_argument('--method', '-m', 
                       choices=['tfidf', 'count', 'hash'],
                       default='tfidf',
                       help='–ú–µ—Ç–æ–¥ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: tfidf)')
    
    parser.add_argument('--input', '-i',
                       default=INPUT_FILE,
                       help=f'–í—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª —Å –¥–∞–Ω–Ω—ã–º–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: {INPUT_FILE})')
    
    parser.add_argument('--text-column', '-t',
                       default=TEXT_COLUMN,
                       help=f'–°—Ç–æ–ª–±–µ—Ü —Å —Ç–µ–∫—Å—Ç–æ–º (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: {TEXT_COLUMN})')
    
    parser.add_argument('--sample', '-s',
                       type=int,
                       default=None,
                       help='–†–∞–∑–º–µ—Ä –≤—ã–±–æ—Ä–∫–∏ (None –¥–ª—è –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö)')
    
    parser.add_argument('--reduce-dim', '-r',
                       action='store_true',
                       help='–£–º–µ–Ω—å—à–∏—Ç—å —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä–æ–≤')
    
    parser.add_argument('--components', '-c',
                       type=int,
                       default=100,
                       help='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç –ø—Ä–∏ —É–º–µ–Ω—å—à–µ–Ω–∏–∏ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 100)')
    
    parser.add_argument('--output-dir', '-o',
                       default='vectorized',
                       help='–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: vectorized)')
    
    args = parser.parse_args()
    
    print("="*60)
    print("–í–ï–ö–¢–û–†–ò–ó–ê–¶–ò–Ø –¢–ï–ö–°–¢–ê")
    print("="*60)
    print(f"–ú–µ—Ç–æ–¥: {VECTORIZATION_METHODS[args.method]['name']}")
    print(f"–í—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª: {args.input}")
    print(f"–°—Ç–æ–ª–±–µ—Ü —Å —Ç–µ–∫—Å—Ç–æ–º: {args.text_column}")
    if args.sample:
        print(f"–†–∞–∑–º–µ—Ä –≤—ã–±–æ—Ä–∫–∏: {args.sample:,}")
    else:
        print("–û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö")
    if args.reduce_dim:
        print(f"–£–º–µ–Ω—å—à–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏: {args.components} –∫–æ–º–ø–æ–Ω–µ–Ω—Ç")
    print(f"–í—ã—Ö–æ–¥–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {args.output_dir}")
    print("="*60)
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    try:
        df = load_data(args.input, args.text_column, sample_size=args.sample)
    except Exception as e:
        print(f"[ERROR] –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –¥–∞–Ω–Ω—ã—Ö: {e}")
        sys.exit(1)
    
    # –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è
    try:
        vectors, vectorizer = vectorize_text(df, args.text_column, method=args.method, output_dir=args.output_dir)
        
        # –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–µ —É–º–µ–Ω—å—à–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
        if args.reduce_dim and vectors.shape[1] > args.components:
            vectors_reduced, reducer = reduce_dimensions(vectors, n_components=args.components)
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —É–º–µ–Ω—å—à–µ–Ω–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤
            reduced_path = os.path.join(args.output_dir, f'vectors_{args.method}_reduced_{args.components}.npy')
            np.save(reduced_path, vectors_reduced)
            print(f"   [SAVE] –£–º–µ–Ω—å—à–µ–Ω–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {reduced_path}")
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–¥—É–∫—Ç–æ—Ä–∞
            reducer_path = os.path.join(args.output_dir, f'reducer_{args.method}_{args.components}.pkl')
            joblib.dump(reducer, reducer_path)
            print(f"   [SAVE] –†–µ–¥—É–∫—Ç–æ—Ä —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {reducer_path}")
        
        print("\n" + "="*60)
        print("[SUCCESS] –í–ï–ö–¢–û–†–ò–ó–ê–¶–ò–Ø –ó–ê–í–ï–†–®–ï–ù–ê –£–°–ü–ï–®–ù–û!")
        print("="*60)
        print(f"\nüìÅ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏: {args.output_dir}/")
        print(f"   - –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä: vectorizer_{args.method}.pkl")
        print(f"   - –í–µ–∫—Ç–æ—Ä—ã (sparse): vectors_{args.method}_sparse.npz")
        if vectors.shape[1] <= 10000:
            print(f"   - –í–µ–∫—Ç–æ—Ä—ã (dense): vectors_{args.method}_dense.npy")
        print(f"   - –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ: metadata_{args.method}.csv")
        if args.method in ['tfidf', 'count']:
            print(f"   - –ü—Ä–∏–∑–Ω–∞–∫–∏: features_{args.method}.csv")
        
    except Exception as e:
        print(f"[ERROR] –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()


