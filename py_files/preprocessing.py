#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
–û—á–∏—Å—Ç–∫–∞ –∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤
"""

import pandas as pd
import re
import os
from collections import Counter
import sys

# –ü–æ–ø—ã—Ç–∫–∞ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å tqdm, –µ—Å–ª–∏ –Ω–µ—Ç - –∏—Å–ø–æ–ª—å–∑—É–µ–º –∑–∞–≥–ª—É—à–∫—É
try:
    from tqdm import tqdm
    HAS_TQDM = True
except ImportError:
    HAS_TQDM = False
    # –ó–∞–≥–ª—É—à–∫–∞ –¥–ª—è tqdm
    class tqdm:
        def __init__(self, *args, **kwargs):
            pass
        def __enter__(self):
            return self
        def __exit__(self, *args):
            pass
        def update(self, n=1):
            pass
        @staticmethod
        def pandas(*args, **kwargs):
            pass

# –£–ø—Ä–æ—â—ë–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –±–µ–∑ NLTK –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏

# ----------------------------
# 1. –ù–∞—Å—Ç—Ä–æ–π–∫–∏
# ----------------------------
INPUT_FILE = "dataset.csv"

# –í—ã—Ö–æ–¥–Ω—ã–µ —Ñ–∞–π–ª—ã
CLEANED_OUTPUT = "dataset_cleaned.csv"
REMOVED_OUTPUT = "dataset_removed.csv"

# –ü–∞–ø–∫–∞ –¥–ª—è –æ—Ç—á–µ—Ç–æ–≤
REPORTS_DIR = "reports"
os.makedirs(REPORTS_DIR, exist_ok=True)
REPORT_CLEANED = os.path.join(REPORTS_DIR, "report_cleaned.txt")
REPORT_REMOVED = os.path.join(REPORTS_DIR, "report_removed.txt")

# ----------------------------
# –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: –¥–æ—Å—Ç–∞–≤–∫–∞ (—Ä–∞—Å—à–∏—Ä–µ–Ω–æ)
# ----------------------------
DELIVERY_KEYWORDS = {
    '–ø–æ—Å—ã–ª–∫', '–¥–æ—Å—Ç–∞–≤–∫', '—Ç—Ä–µ–∫', '–æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–∏', '–ø–æ–ª—É—á', '–∑–∞–±—Ä–∞—Ç—å', '–ø—É–Ω–∫—Ç', '–≤—ã–¥–∞—á',
    '—Å—Ç–∞—Ç—É—Å', '–∑–∞–¥–µ—Ä–∂', '–ø–æ—Ç–µ—Ä—è', '–ø–æ–≤—Ä–µ–∂–¥', '–≤–æ–∑–≤—Ä–∞—Ç', '–æ—Ç–ø—Ä–∞–≤', '–∞–¥—Ä–µ—Å', '–∫—É—Ä—å–µ—Ä',
    '–ø–æ—á—Ç–∞', '5post', '–ø—è—Ç–µ—Ä–æ—á–∫', '–ø–µ—Ä–µ–∫—Ä–µ—Å—Ç–æ–∫', '–º–∞–≥–∞–∑–∏–Ω', '—è—â–∏–∫', '–ª–æ–∫–µ—Ä',
    '–ø—Ä–æ–±–ª–µ–º', '–∂–∞–ª–æ–±', '–æ—à–∏–±–∫', '–∏–∑–º–µ–Ω–∏', '–æ—Ç–º–µ–Ω', '–∑–∞–∫–∞–∑', '—Ç–æ–≤–∞—Ä', '–±—Ä–∞–∫',
    '–∂–¥–∞—Ç—å', '–ø—Ä–∏–¥–µ—Ç', '–ø—Ä–∏–¥—ë—Ç', '—Å–∫–æ–ª—å–∫–æ', '–≥–¥–µ', '–ø–æ—á–µ–º—É', '–∫–æ–≥–¥–∞', '–æ—Ç—Å–ª–µ–¥–∏—Ç—å',
    # --- –î–û–ë–ê–í–õ–ï–ù–û –¥–ª—è –ø–æ—Å—Ç–∞–º–∞—Ç–æ–≤ ---
    '–ø–æ—Å—Ç–∞–º–∞—Ç', '–∞–≤—Ç–æ–º–∞—Ç', '—Ç–µ—Ä–º–∏–Ω–∞–ª', '—è—á–µ–π–∫', '–ª–æ–∫–µ—Ä', '–æ—Ç–∫—Ä—ã', '–ø—É—Å—Ç', '–∏–Ω–≤–µ–Ω—Ç–∞—Ä–∏–∑–∞—Ü', '—Å—Ä–æ–∫ —Ö—Ä–∞–Ω–µ–Ω–∏—è'
}

# –ó–∞—â–∏—Ç–∞ —á–∞—Å—Ç—ã—Ö –∂–∞–ª–æ–±
PROTECTED_PHRASES = {
    '–Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç –ø–æ—Å—Ç–∞–º–∞—Ç',
    '–Ω–µ –æ—Ç–∫—Ä—ã–ª–∞—Å—å —è—á–µ–π–∫–∞',
    '–Ω–µ –æ—Ç–∫—Ä—ã–≤–∞–µ—Ç—Å—è —è—á–µ–π–∫–∞',
    '–ø—Ä–æ–¥–ª–∏—Ç—å —Å—Ä–æ–∫ —Ö—Ä–∞–Ω–µ–Ω–∏—è',
    '–Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç –∞–≤—Ç–æ–º–∞—Ç',
    '–ø—É—Å—Ç–∞—è —è—á–µ–π–∫–∞',
    '–∏–Ω–≤–µ–Ω—Ç–∞—Ä–∏–∑–∞—Ü–∏—è –ø–æ—Å—Ç–∞–º–∞—Ç–∞',
    '–ø–æ—Å—Ç–∞–º–∞—Ç –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç',
    '—è—á–µ–π–∫–∞ –Ω–µ –æ—Ç–∫—Ä—ã–ª–∞—Å—å',
    '–Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç —Ç–µ—Ä–º–∏–Ω–∞–ª',
    '–∞–≤—Ç–æ–º–∞—Ç –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç',
    '—è—á–µ–π–∫–∞ –ø—É—Å—Ç–∞—è'
}

NON_INFORMATIVE_PHRASES = {"—Å–ø–∞—Å–∏–±–æ", "–æ–∫", "–¥–∞", "–Ω–µ—Ç", "–±–æ–ª—å—à–æ–µ", "–±–æ–ª—å—à–æ–µ —Å–ø–∞—Å–∏–±–æ"}

OPERATOR_KEYWORDS = {
    '–æ–ø–µ—Ä–∞—Ç–æ—Ä', '–º–µ–Ω–µ–¥–∂–µ—Ä', '–∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç', '–ø–æ–º–æ—â—å', '–ø–æ–¥–¥–µ—Ä–∂–∫–∞', 
    '—Å–≤—è–∑–∞—Ç—å—Å—è', '–ø–æ–∑–≤–æ–Ω–∏—Ç—å', '–ø–µ—Ä–µ–≤–µ—Å—Ç–∏', '—Å–æ–µ–¥–∏–Ω–∏—Ç—å',
    '—Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç', '—á–µ–ª–æ–≤–µ–∫', '—Ä–∞–±–æ—Ç–Ω–∏–∫', '—Å–æ—Ç—Ä—É–¥–Ω–∏–∫', '–∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—è', 
    '–º–µ—Å—Ç–æ–Ω–∞—Ö–æ–∂–¥–µ–Ω–∏–µ', '—Ç–µ—Ö–ø–æ–¥–¥–µ—Ä–∂–∫–∞', '–º–µ—Å—Ç–æ–Ω–∞—Ö–æ–∂–¥–µ–Ω–∏', '—Ç–µ—Ö–ø–æ–¥–¥–µ—Ä–∂–∫'
}


def classify_relevance(text):
    if not isinstance(text, str) or not text.strip():
        return False, "empty_or_not_string"
    
    text_clean = text.strip().lower()
    
    # üîí –ó–∞—â–∏—Ç–∞ –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ñ—Ä–∞–∑
    if text_clean in PROTECTED_PHRASES:
        return True, "protected_phrase"
    
    if len(text_clean) <= 2:
        return False, "too_short"
    
    words = text_clean.split()

    if text_clean in NON_INFORMATIVE_PHRASES:
        return False, "exact_greeting"
    
    if all(word in NON_INFORMATIVE_PHRASES for word in words):
        return False, "all_non_informative"
    
    if len(words) <= 2 and all(word in NON_INFORMATIVE_PHRASES for word in words):
        return False, "short_non_informative"

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
    has_delivery = any(kw in word for word in words for kw in DELIVERY_KEYWORDS)
    has_operator = any(kw in text_clean for kw in OPERATOR_KEYWORDS)

    if has_delivery and has_operator:
        return True, "delivery_and_operator"
    elif has_delivery:
        return True, "delivery_related"
    elif has_operator:
        return True, "operator_request"
    
    if len(words) > 4:
        return True, "long_phrase"
    
    return False, "no_relevant_content"


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    # ----------------------------
    # 2. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö (—Å –±–∞—Ç—á-–æ–±—Ä–∞–±–æ—Ç–∫–æ–π –¥–ª—è –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–æ–≤)
    # ----------------------------
    if not os.path.exists(INPUT_FILE):
        raise FileNotFoundError(f"–§–∞–π–ª {INPUT_FILE} –Ω–µ –Ω–∞–π–¥–µ–Ω –≤: {os.getcwd()}")

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏
    file_size_mb = os.path.getsize(INPUT_FILE) / (1024 * 1024)
    print(f"–†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: {file_size_mb:.1f} MB")

    # –î–ª—è —Ñ–∞–π–ª–æ–≤ –±–æ–ª—å—à–µ 500MB –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–∞—Ç—á-–æ–±—Ä–∞–±–æ—Ç–∫—É
    USE_BATCH = file_size_mb > 500
    BATCH_SIZE = 50000  # —Å—Ç—Ä–æ–∫ –Ω–∞ –±–∞—Ç—á

    if USE_BATCH:
        print(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –±–∞—Ç—á-–æ–±—Ä–∞–±–æ—Ç–∫–∞ (–±–∞—Ç—á = {BATCH_SIZE} —Å—Ç—Ä–æ–∫)")
        # –°–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∂–∞–µ–º –Ω–µ–±–æ–ª—å—à—É—é –≤—ã–±–æ—Ä–∫—É –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
        df_sample = pd.read_csv(INPUT_FILE, nrows=100)
        text_col = 'text' if 'text' in df_sample.columns else df_sample.columns[1] if len(df_sample.columns) > 1 else df_sample.columns[0]
        print(f"–û–ø—Ä–µ–¥–µ–ª—ë–Ω —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Å—Ç–æ–ª–±–µ—Ü: '{text_col}'")
        print(f"–ù–∞—á–∏–Ω–∞–µ–º –±–∞—Ç—á-–æ–±—Ä–∞–±–æ—Ç–∫—É...")
    else:
        print("–ó–∞–≥—Ä—É–∑–∫–∞ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö –≤ –ø–∞–º—è—Ç—å...")
        df = pd.read_csv(INPUT_FILE)
        print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} —Å—Ç—Ä–æ–∫. –°—Ç–æ–ª–±—Ü—ã: {list(df.columns)}")
        text_col = 'text' if 'text' in df.columns else df.columns[1] if len(df.columns) > 1 else df.columns[0]
        if text_col not in df.columns:
            raise ValueError(f"–û–∂–∏–¥–∞–µ–º—ã–π —Å—Ç–æ–ª–±–µ—Ü '{text_col}' –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç.")

    # ----------------------------
    # 3. –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
    # ----------------------------
    def basic_clean(text):
        if not isinstance(text, str):
            return ""
        text = text.lower().strip()
        text = re.sub(r'[^\w\s]', ' ', text)
        text = re.sub(r'\s+', ' ', text)
        return text

    if USE_BATCH:
        # –ë–∞—Ç—á-–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–æ–≤
        all_cleaned = []
        all_removed = []
        total_rows = sum(1 for _ in open(INPUT_FILE)) - 1  # -1 –¥–ª—è –∑–∞–≥–æ–ª–æ–≤–∫–∞
        
        with tqdm(total=total_rows, desc="–û–±—Ä–∞–±–æ—Ç–∫–∞") as pbar:
            for chunk in pd.read_csv(INPUT_FILE, chunksize=BATCH_SIZE):
                # –û—á–∏—Å—Ç–∫–∞
                chunk['cleaned'] = chunk[text_col].apply(basic_clean)
                chunk['cleaned_for_classification'] = chunk['cleaned']
                
                # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
                results = chunk['cleaned_for_classification'].apply(lambda x: classify_relevance(x))
                chunk['is_relevant'] = results.apply(lambda x: x[0])
                chunk['removal_reason'] = results.apply(lambda x: x[1])
                
                # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ
                df_cleaned_chunk = chunk[chunk['is_relevant']].copy()
                df_removed_chunk = chunk[~chunk['is_relevant']].copy()
                
                all_cleaned.append(df_cleaned_chunk)
                all_removed.append(df_removed_chunk)
                
                pbar.update(len(chunk))
        
        # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        print("–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...")
        df_cleaned = pd.concat(all_cleaned, ignore_index=True)
        df_removed = pd.concat(all_removed, ignore_index=True)
        
        print(f"‚úÖ –û—Å—Ç–∞–≤–ª–µ–Ω–æ: {len(df_cleaned)}")
        print(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω–æ: {len(df_removed)}")
        
        # –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        print("–£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤...")
        before_dedup = len(df_cleaned)
        df_cleaned = df_cleaned.drop_duplicates(subset=['cleaned'])
        after_dedup = len(df_cleaned)
        print(f"–£–¥–∞–ª–µ–Ω–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {before_dedup - after_dedup}")
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
        print("–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...")
        df_cleaned.to_csv(CLEANED_OUTPUT, index=False, encoding='utf-8')
        df_removed.to_csv(REMOVED_OUTPUT, index=False, encoding='utf-8')
        
    else:
        # –û–±—ã—á–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è –Ω–µ–±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–æ–≤
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ –±–∞–∑–æ–≤—É—é –æ—á–∏—Å—Ç–∫—É (–±—ã—Å—Ç—Ä–µ–µ –≤ —Ä–∞–∑—ã!)
        print("–û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞...")
        if HAS_TQDM:
            tqdm.pandas(desc="–û—á–∏—Å—Ç–∫–∞")
            df['cleaned'] = df[text_col].progress_apply(basic_clean)
        else:
            df['cleaned'] = df[text_col].apply(basic_clean)
        df['cleaned_for_classification'] = df['cleaned']

        # ----------------------------
        # 4. –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ (—Å –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–º)
        # ----------------------------
        print("–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤...")
        if HAS_TQDM:
            tqdm.pandas(desc="–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è")
            results = df['cleaned_for_classification'].progress_apply(lambda x: classify_relevance(x))
        else:
            results = df['cleaned_for_classification'].apply(lambda x: classify_relevance(x))
        df['is_relevant'] = results.apply(lambda x: x[0])
        df['removal_reason'] = results.apply(lambda x: x[1])

        # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ
        df_cleaned = df[df['is_relevant']].copy()
        df_removed = df[~df['is_relevant']].copy()

        print(f"‚úÖ –û—Å—Ç–∞–≤–ª–µ–Ω–æ: {len(df_cleaned)}")
        print(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω–æ: {len(df_removed)}")

        # ----------------------------
        # 5. –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ (—Ç–æ–ª—å–∫–æ –≤ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö)
        # ----------------------------
        print("–£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤...")
        before_dedup = len(df_cleaned)
        df_cleaned = df_cleaned.drop_duplicates(subset=['cleaned'])  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—É—é –æ—á–∏—Å—Ç–∫—É
        after_dedup = len(df_cleaned)
        print(f"–£–¥–∞–ª–µ–Ω–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {before_dedup - after_dedup}")

        # ----------------------------
        # 6. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
        # ----------------------------
        df_cleaned.to_csv(CLEANED_OUTPUT, index=False, encoding='utf-8')
        df_removed.to_csv(REMOVED_OUTPUT, index=False, encoding='utf-8')

    # ----------------------------
    # 7. –û—Ç—á—ë—Ç—ã
    # ----------------------------
    # –û—Ç—á—ë—Ç: –æ—á–∏—â–µ–Ω–Ω—ã–µ
    report_clean = [
        "=== –û–¢–ß–Å–¢: –†–ï–õ–ï–í–ê–ù–¢–ù–´–ï –ó–ê–ü–†–û–°–´ ===\n",
        f"–í—Å–µ–≥–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö: {len(df_cleaned)}",
        f"–ü–æ—Å–ª–µ –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏: {after_dedup}\n",
        "–ú–µ—Ç–∫–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç ‚Äî —Ç—Ä–µ–±—É–µ—Ç—Å—è —Ä–∞–∑–º–µ—Ç–∫–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏.\n",
        "–£–ü–†–û–©–Å–ù–ù–ê–Ø –û–ë–†–ê–ë–û–¢–ö–ê:",
        "‚úì –ë–∞–∑–æ–≤–∞—è –æ—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ (–±—ã—Å—Ç—Ä–æ)",
        "‚úì –£–¥–∞–ª–µ–Ω–∏–µ –∑–Ω–∞–∫–æ–≤ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è",
        "‚úì –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–±–µ–ª–æ–≤",
        "‚úì –ö–æ–ª–æ–Ω–∫–∞ 'cleaned' —Å–æ–¥–µ—Ä–∂–∏—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç"
    ]

    # –û—Ç—á—ë—Ç: —É–¥–∞–ª—ë–Ω–Ω—ã–µ
    reason_counts = Counter(df_removed['removal_reason'])
    top_phrases = Counter(df_removed[text_col].fillna("").astype(str)).most_common(20)

    report_removed = ["=== –û–¢–ß–Å–¢: –£–î–ê–õ–Å–ù–ù–´–ï –ó–ê–ü–†–û–°–´ ===\n", f"–í—Å–µ–≥–æ —É–¥–∞–ª–µ–Ω–æ: {len(df_removed)}\n"]
    report_removed.append("–ü—Ä–∏—á–∏–Ω—ã —É–¥–∞–ª–µ–Ω–∏—è:")
    for reason, cnt in reason_counts.most_common():
        report_removed.append(f"  {reason}: {cnt}")

    report_removed.append("\n–¢–æ–ø-20 —É–¥–∞–ª—ë–Ω–Ω—ã—Ö —Ñ—Ä–∞–∑:")
    for phrase, cnt in top_phrases:
        if phrase.strip():
            report_removed.append(f"  '{phrase}' ‚Äî {cnt}")

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á—ë—Ç–æ–≤
    with open(REPORT_CLEANED, 'w', encoding='utf-8') as f:
        f.write('\n'.join(report_clean))

    with open(REPORT_REMOVED, 'w', encoding='utf-8') as f:
        f.write('\n'.join(report_removed))

    print(f"\n‚úÖ –ì–æ—Ç–æ–≤–æ!")
    print(f"–†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ ‚Üí {CLEANED_OUTPUT}")
    print(f"–£–¥–∞–ª—ë–Ω–Ω—ã–µ ‚Üí {REMOVED_OUTPUT}")
    print(f"–û—Ç—á—ë—Ç—ã ‚Üí {REPORT_CLEANED}, {REPORT_REMOVED}")


if __name__ == "__main__":
    main()



