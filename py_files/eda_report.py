#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
EDA (Exploratory Data Analysis) –æ—Ç—á—ë—Ç –¥–ª—è dataset.csv
–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–π –∏–∑ –¥–∞–Ω–Ω—ã—Ö
"""

import pandas as pd
import os
from collections import Counter
import re
from datetime import datetime
import sys

# –ü–æ–ø—ã—Ç–∫–∞ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å sklearn (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
try:
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.cluster import KMeans
    import numpy as np
    HAS_SKLEARN = True
except ImportError:
    HAS_SKLEARN = False
    print("–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: sklearn –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –ù–µ–∫–æ—Ç–æ—Ä—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –∞–Ω–∞–ª–∏–∑–∞ –±—É–¥—É—Ç –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã.")
    print("–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install scikit-learn numpy")

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏
INPUT_FILE = "dataset.csv"

# –ü–∞–ø–∫–∞ –¥–ª—è –æ—Ç—á–µ—Ç–æ–≤
REPORTS_DIR = "reports"
os.makedirs(REPORTS_DIR, exist_ok=True)
OUTPUT_REPORT = os.path.join(REPORTS_DIR, "eda_report.txt")

print("–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è EDA...")

try:
    df_sample = pd.read_csv(INPUT_FILE, nrows=10000)
    print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–∞ –≤—ã–±–æ—Ä–∫–∞: {len(df_sample)} —Å—Ç—Ä–æ–∫")
    print(f"–°—Ç–æ–ª–±—Ü—ã: {list(df_sample.columns)}")
    
    total_rows = sum(1 for _ in open(INPUT_FILE)) - 1  # -1 –¥–ª—è –∑–∞–≥–æ–ª–æ–≤–∫–∞
    print(f"–í—Å–µ–≥–æ —Å—Ç—Ä–æ–∫ –≤ —Ñ–∞–π–ª–µ: {total_rows:,}")
    
except Exception as e:
    print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ: {e}")
    sys.exit(1)

text_col = 'text' if 'text' in df_sample.columns else df_sample.columns[1] if len(df_sample.columns) > 1 else df_sample.columns[0]
datetime_col = 'datetime' if 'datetime' in df_sample.columns else None

print(f"\n–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Å—Ç–æ–ª–±–µ—Ü: '{text_col}'")

report_lines = []
report_lines.append("EDA –û–¢–ß–Å–¢")
report_lines.append("")

report_lines.append("="*60)
report_lines.append(" –ë–ê–ó–û–í–ê–Ø –ò–ù–§–û–†–ú–ê–¶–ò–Ø –û –î–ê–ù–ù–´–•")
report_lines.append("="*60)
report_lines.append(f"–í—Å–µ–≥–æ —Å—Ç—Ä–æ–∫ –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ: {total_rows:,}")
report_lines.append(f"–°—Ç–æ–ª–±—Ü—ã: {', '.join(df_sample.columns)}")
report_lines.append(f"–†–∞–∑–º–µ—Ä –≤—ã–±–æ—Ä–∫–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞: {len(df_sample):,} —Å—Ç—Ä–æ–∫")
report_lines.append("")

report_lines.append("="*60)
report_lines.append(" –ê–ù–ê–õ–ò–ó –¢–ï–ö–°–¢–û–í–û–ì–û –°–¢–û–õ–ë–¶–ê")
report_lines.append("="*60)
null_count = df_sample[text_col].isna().sum()
report_lines.append(f"–ü—É—Å—Ç—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π: {null_count} ({null_count/len(df_sample)*100:.2f}%)")
text_lengths = df_sample[text_col].fillna("").astype(str).str.len()
report_lines.append(f"\n–î–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–æ–≤:")
report_lines.append(f"  –ú–∏–Ω–∏–º—É–º: {text_lengths.min()} —Å–∏–º–≤–æ–ª–æ–≤")
report_lines.append(f"  –ú–∞–∫—Å–∏–º—É–º: {text_lengths.max()} —Å–∏–º–≤–æ–ª–æ–≤")
report_lines.append(f"  –°—Ä–µ–¥–Ω–µ–µ: {text_lengths.mean():.1f} —Å–∏–º–≤–æ–ª–æ–≤")
report_lines.append(f"  –ú–µ–¥–∏–∞–Ω–∞: {text_lengths.median():.1f} —Å–∏–º–≤–æ–ª–æ–≤")
report_lines.append(f"  Q1 (25%): {text_lengths.quantile(0.25):.1f} —Å–∏–º–≤–æ–ª–æ–≤")
report_lines.append(f"  Q3 (75%): {text_lengths.quantile(0.75):.1f} —Å–∏–º–≤–æ–ª–æ–≤")

word_counts = df_sample[text_col].fillna("").astype(str).str.split().str.len()
report_lines.append(f"\n–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤:")
report_lines.append(f"  –ú–∏–Ω–∏–º—É–º: {word_counts.min()} —Å–ª–æ–≤")
report_lines.append(f"  –ú–∞–∫—Å–∏–º—É–º: {word_counts.max()} —Å–ª–æ–≤")
report_lines.append(f"  –°—Ä–µ–¥–Ω–µ–µ: {word_counts.mean():.1f} —Å–ª–æ–≤")
report_lines.append(f"  –ú–µ–¥–∏–∞–Ω–∞: {word_counts.median():.1f} —Å–ª–æ–≤")

report_lines.append("\n" + "="*60)
report_lines.append("–¢–û–ü-20 –°–ê–ú–´–• –ß–ê–°–¢–´–• –§–†–ê–ó")
report_lines.append("="*60)

top_phrases = Counter(df_sample[text_col].fillna("").astype(str).str.lower().str.strip()).most_common(20)
for i, (phrase, count) in enumerate(top_phrases, 1):
    if phrase.strip():
        report_lines.append(f"  {i:2d}. '{phrase[:60]}...' ({count} —Ä–∞–∑)")

# –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–û–ï –û–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ö–ê–¢–ï–ì–û–†–ò–ô
print("\n[–ê–Ω–∞–ª–∏–∑] –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–π...")

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞
def preprocess_text(text):
    if not isinstance(text, str):
        return ""
    text = text.lower().strip()
    # –£–¥–∞–ª—è–µ–º –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è, –æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –±—É–∫–≤—ã –∏ —Ü–∏—Ñ—Ä—ã
    text = re.sub(r'[^\w\s]', ' ', text)
    text = re.sub(r'\s+', ' ', text)
    return text

# –†—É—Å—Å–∫–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ (—á–∞—Å—Ç—ã–µ —Å–ª–æ–≤–∞, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ –Ω–µ—Å—É—Ç —Å–º—ã—Å–ª–æ–≤–æ–π –Ω–∞–≥—Ä—É–∑–∫–∏)
STOP_WORDS = {
    '–∏', '–≤', '–≤–æ', '–Ω–µ', '—á—Ç–æ', '–æ–Ω', '–Ω–∞', '—è', '—Å', '—Å–æ', '–∫–∞–∫', '–∞', '—Ç–æ', '–≤—Å–µ',
    '–æ–Ω–∞', '—Ç–∞–∫', '–µ–≥–æ', '–Ω–æ', '–¥–∞', '—Ç—ã', '–∫', '—É', '–∂–µ', '–≤—ã', '–∑–∞', '–±—ã', '–ø–æ',
    '—Ç–æ–ª—å–∫–æ', '–µ–µ', '–º–Ω–µ', '–±—ã–ª–æ', '–≤–æ—Ç', '–æ—Ç', '–º–µ–Ω—è', '–µ—â–µ', '–Ω–µ—Ç', '–æ', '–∏–∑',
    '–µ–º—É', '—Ç–µ–ø–µ—Ä—å', '–∫–æ–≥–¥–∞', '–¥–∞–∂–µ', '–Ω—É', '–≤–¥—Ä—É–≥', '–ª–∏', '–µ—Å–ª–∏', '—É–∂–µ', '–∏–ª–∏',
    '–Ω–∏', '–±—ã—Ç—å', '–±—ã–ª', '–Ω–µ–≥–æ', '–¥–æ', '–≤–∞—Å', '–Ω–∏–±—É–¥—å', '–æ–ø—è—Ç—å', '—É–∂', '–≤–∞–º', '–≤–µ–¥—å',
    '—Ç–∞–º', '–ø–æ—Ç–æ–º', '—Å–µ–±—è', '–Ω–∏—á–µ–≥–æ', '–µ–π', '–º–æ–∂–µ—Ç', '–æ–Ω–∏', '—Ç—É—Ç', '–≥–¥–µ', '–µ—Å—Ç—å',
    '–Ω–∞–¥–æ', '–Ω–µ–π', '–¥–ª—è', '–º—ã', '—Ç–µ–±—è', '–∏—Ö', '—á–µ–º', '–±—ã–ª–∞', '—Å–∞–º', '—á—Ç–æ–±', '–±–µ–∑',
    '–±—É–¥—Ç–æ', '—á–µ–≥–æ', '—Ä–∞–∑', '—Ç–æ–∂–µ', '—Å–µ–±–µ', '–ø–æ–¥', '–±—É–¥–µ—Ç', '–∂', '—Ç–æ–≥–¥–∞', '–∫—Ç–æ',
    '—ç—Ç–æ—Ç', '—Ç–æ–≥–æ', '–ø–æ—Ç–æ–º—É', '—ç—Ç–æ–≥–æ', '–∫–∞–∫–æ–π', '—Å–æ–≤—Å–µ–º', '–Ω–∏–º', '–∑–¥–µ—Å—å', '—ç—Ç–æ–º',
    '–æ–¥–∏–Ω', '–ø–æ—á—Ç–∏', '–º–æ–π', '—Ç–µ–º', '—á—Ç–æ–±—ã', '–Ω–µ–µ', '—Å–µ–π—á–∞—Å', '–±—ã–ª–∏', '–∫—É–¥–∞', '–∑–∞—á–µ–º',
    '–≤—Å–µ—Ö', '–Ω–∏–∫–æ–≥–¥–∞', '–º–æ–∂–Ω–æ', '–ø—Ä–∏', '–Ω–∞–∫–æ–Ω–µ—Ü', '–¥–≤–∞', '–æ–±', '–¥—Ä—É–≥–æ–π', '—Ö–æ—Ç—å',
    '–ø–æ—Å–ª–µ', '–Ω–∞–¥', '–±–æ–ª—å—à–µ', '—Ç–æ—Ç', '—á–µ—Ä–µ–∑', '—ç—Ç–∏', '–Ω–∞—Å', '–ø—Ä–æ', '–≤—Å–µ–≥–æ', '–Ω–∏—Ö',
    '–∫–∞–∫–∞—è', '–º–Ω–æ–≥–æ', '—Ä–∞–∑–≤–µ', '—Ç—Ä–∏', '—ç—Ç—É', '–º–æ—è', '–≤–ø—Ä–æ—á–µ–º', '—Ö–æ—Ä–æ—à–æ', '—Å–≤–æ—é',
    '—ç—Ç–æ–π', '–ø–µ—Ä–µ–¥', '–∏–Ω–æ–≥–¥–∞', '–ª—É—á—à–µ', '—á—É—Ç—å', '—Ç–æ–º', '–Ω–µ–ª—å–∑—è', '—Ç–∞–∫–æ–π', '–∏–º',
    '–±–æ–ª–µ–µ', '–≤—Å–µ–≥–¥–∞', '–∫–æ–Ω–µ—á–Ω–æ', '–≤—Å—é', '–º–µ–∂–¥—É'
}

# –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤
print("  [1/5] –û—á–∏—Å—Ç–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤...")
texts_clean = df_sample[text_col].fillna("").astype(str).apply(preprocess_text)
texts_clean = texts_clean[texts_clean.str.len() > 2]  # –£–±–∏—Ä–∞–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ

# –ê–Ω–∞–ª–∏–∑ —á–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç–∏ —Å–ª–æ–≤
print("  [2/5] –ê–Ω–∞–ª–∏–∑ —á–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç–∏ —Å–ª–æ–≤...")
all_words = []
for text in texts_clean:
    words = text.split()
    # –§–∏–ª—å—Ç—Ä—É–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ –∏ –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–ª–æ–≤–∞
    words = [w for w in words if len(w) > 2 and w not in STOP_WORDS]
    all_words.extend(words)

word_freq = Counter(all_words)

# –ê–Ω–∞–ª–∏–∑ –±–∏–≥—Ä–∞–º–º (–¥–≤—É—Ö—Å–ª–æ–≤–Ω—ã—Ö —Ñ—Ä–∞–∑)
print("  [3/5] –ê–Ω–∞–ª–∏–∑ —Ñ—Ä–∞–∑ (–±–∏–≥—Ä–∞–º–º—ã –∏ —Ç—Ä–∏–≥—Ä–∞–º–º—ã)...")
bigrams = []
trigrams = []
for text in texts_clean:
    words = [w for w in text.split() if len(w) > 2 and w not in STOP_WORDS]
    # –ë–∏–≥—Ä–∞–º–º—ã
    for i in range(len(words) - 1):
        bigrams.append(f"{words[i]} {words[i+1]}")
    # –¢—Ä–∏–≥—Ä–∞–º–º—ã
    for i in range(len(words) - 2):
        trigrams.append(f"{words[i]} {words[i+1]} {words[i+2]}")

bigram_freq = Counter(bigrams)
trigram_freq = Counter(trigrams)

# TF-IDF –∞–Ω–∞–ª–∏–∑ –¥–ª—è –≤—ã–¥–µ–ª–µ–Ω–∏—è –∑–Ω–∞—á–∏–º—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤
print("  [4/5] TF-IDF –∞–Ω–∞–ª–∏–∑ –¥–ª—è –≤—ã–¥–µ–ª–µ–Ω–∏—è –∫–ª—é—á–µ–≤—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤...")
if HAS_SKLEARN:
    try:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 5000 —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è TF-IDF (–¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏)
        texts_for_tfidf = texts_clean.head(5000).tolist()
        if len(texts_for_tfidf) > 100:
            vectorizer = TfidfVectorizer(max_features=100, ngram_range=(1, 2), min_df=2, max_df=0.95)
            tfidf_matrix = vectorizer.fit_transform(texts_for_tfidf)
            feature_names = vectorizer.get_feature_names_out()
            
            # –ü–æ–ª—É—á–∞–µ–º —Å—Ä–µ–¥–Ω–∏–µ TF-IDF –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–µ—Ä–º–∏–Ω–∞
            mean_tfidf = np.mean(tfidf_matrix.toarray(), axis=0)
            top_tfidf_indices = np.argsort(mean_tfidf)[-30:][::-1]  # –¢–æ–ø-30
            top_tfidf_terms = [(feature_names[i], mean_tfidf[i]) for i in top_tfidf_indices]
        else:
            top_tfidf_terms = []
    except Exception as e:
        print(f"    –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: TF-IDF –∞–Ω–∞–ª–∏–∑ –Ω–µ –≤—ã–ø–æ–ª–Ω–µ–Ω ({e})")
        top_tfidf_terms = []
else:
    print("    –ü—Ä–æ–ø—É—â–µ–Ω–æ: —Ç—Ä–µ–±—É–µ—Ç—Å—è sklearn")
    top_tfidf_terms = []

# –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è –æ—Å–Ω–æ–≤–Ω—ã—Ö —Ç–µ–º
print("  [5/5] –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è —Ç–µ–º...")
texts_for_cluster = []  # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –æ—Ç—á–µ—Ç–µ
if HAS_SKLEARN:
    try:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤—ã–±–æ—Ä–∫—É –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
        texts_for_cluster = texts_clean.head(2000).tolist()
        if len(texts_for_cluster) > 50:
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ (–æ—Ç 3 –¥–æ 10)
            n_clusters = min(10, max(3, len(texts_for_cluster) // 200))
            
            cluster_vectorizer = TfidfVectorizer(max_features=50, ngram_range=(1, 2), min_df=2)
            cluster_matrix = cluster_vectorizer.fit_transform(texts_for_cluster)
            
            kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
            clusters = kmeans.fit_predict(cluster_matrix)
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞
            cluster_keywords = {}
            feature_names_cluster = cluster_vectorizer.get_feature_names_out()
            
            for i in range(n_clusters):
                cluster_center = kmeans.cluster_centers_[i]
                top_indices = np.argsort(cluster_center)[-5:][::-1]
                top_words = [feature_names_cluster[idx] for idx in top_indices]
                cluster_keywords[i] = {
                    'keywords': top_words,
                    'size': int(np.sum(clusters == i))
                }
        else:
            cluster_keywords = {}
    except Exception as e:
        print(f"    –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –Ω–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∞ ({e})")
        cluster_keywords = {}
else:
    print("    –ü—Ä–æ–ø—É—â–µ–Ω–æ: —Ç—Ä–µ–±—É–µ—Ç—Å—è sklearn")
    cluster_keywords = {}

# –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç—á—ë—Ç–∞ –æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è—Ö
report_lines.append("\n" + "="*60)
report_lines.append(" –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–ò –û–ü–†–ï–î–ï–õ–Å–ù–ù–´–ï –ö–ê–¢–ï–ì–û–†–ò–ò")
report_lines.append("="*60)

# –¢–æ–ø –∑–Ω–∞—á–∏–º—ã—Ö —Å–ª–æ–≤
report_lines.append("\nüìä –¢–û–ü-30 –°–ê–ú–´–• –ß–ê–°–¢–´–• –ó–ù–ê–ß–ò–ú–´–• –°–õ–û–í:")
top_words = word_freq.most_common(30)
for i, (word, count) in enumerate(top_words, 1):
    pct = count / len(texts_clean) * 100 if len(texts_clean) > 0 else 0
    report_lines.append(f"  {i:2d}. '{word}': {count} —Ä–∞–∑ ({pct:.1f}% —Ç–µ–∫—Å—Ç–æ–≤)")

# –¢–æ–ø –±–∏–≥—Ä–∞–º–º
report_lines.append("\nüìù –¢–û–ü-20 –°–ê–ú–´–• –ß–ê–°–¢–´–• –§–†–ê–ó (2 —Å–ª–æ–≤–∞):")
top_bigrams = bigram_freq.most_common(20)
for i, (phrase, count) in enumerate(top_bigrams, 1):
    pct = count / len(texts_clean) * 100 if len(texts_clean) > 0 else 0
    report_lines.append(f"  {i:2d}. '{phrase}': {count} —Ä–∞–∑ ({pct:.1f}%)")

# –¢–æ–ø —Ç—Ä–∏–≥—Ä–∞–º–º
report_lines.append("\nüìù –¢–û–ü-15 –°–ê–ú–´–• –ß–ê–°–¢–´–• –§–†–ê–ó (3 —Å–ª–æ–≤–∞):")
top_trigrams = trigram_freq.most_common(15)
for i, (phrase, count) in enumerate(top_trigrams, 1):
    pct = count / len(texts_clean) * 100 if len(texts_clean) > 0 else 0
    report_lines.append(f"  {i:2d}. '{phrase}': {count} —Ä–∞–∑ ({pct:.1f}%)")

# TF-IDF –∫–ª—é—á–µ–≤—ã–µ —Ç–µ—Ä–º–∏–Ω—ã
if top_tfidf_terms:
    report_lines.append("\nüîë –ö–õ–Æ–ß–ï–í–´–ï –¢–ï–†–ú–ò–ù–´ (TF-IDF –∞–Ω–∞–ª–∏–∑, —Ç–æ–ø-20):")
    for i, (term, score) in enumerate(top_tfidf_terms[:20], 1):
        report_lines.append(f"  {i:2d}. '{term}': {score:.4f}")

# –ö–ª–∞—Å—Ç–µ—Ä—ã (–∫–∞—Ç–µ–≥–æ—Ä–∏–∏)
if cluster_keywords:
    report_lines.append("\nüéØ –û–°–ù–û–í–ù–´–ï –¢–ï–ú–ê–¢–ò–ß–ï–°–ö–ò–ï –ö–ê–¢–ï–ì–û–†–ò–ò (–∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è):")
    sorted_clusters = sorted(cluster_keywords.items(), key=lambda x: x[1]['size'], reverse=True)
    for cluster_id, info in sorted_clusters:
        keywords_str = ", ".join(info['keywords'])
        pct = info['size'] / len(texts_for_cluster) * 100 if len(texts_for_cluster) > 0 else 0
        report_lines.append(f"  –ö–∞—Ç–µ–≥–æ—Ä–∏—è {cluster_id+1}: {info['size']} —Ç–µ–∫—Å—Ç–æ–≤ ({pct:.1f}%)")
        report_lines.append(f"    –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: {keywords_str}")

# –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
auto_keywords = set()
# –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ–ø-50 —Å–ª–æ–≤
auto_keywords.update([w for w, _ in word_freq.most_common(50)])
# –î–æ–±–∞–≤–ª—è–µ–º —Å–ª–æ–≤–∞ –∏–∑ —Ç–æ–ø –±–∏–≥—Ä–∞–º–º
for phrase, _ in bigram_freq.most_common(30):
    auto_keywords.update(phrase.split())

report_lines.append(f"\nüí° –í—Å–µ–≥–æ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤—ã–¥–µ–ª–µ–Ω–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤: {len(auto_keywords)}")

# –ê–Ω–∞–ª–∏–∑ –ø–æ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã–º –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
report_lines.append("\nüìà –ê–ù–ê–õ–ò–ó –ü–û –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–ò –û–ü–†–ï–î–ï–õ–Å–ù–ù–´–ú –ö–õ–Æ–ß–ï–í–´–ú –°–õ–û–í–ê–ú (—Ç–æ–ø-25):")
keyword_counts_auto = {}
for keyword in list(auto_keywords)[:25]:  # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 25 –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
    count = df_sample[text_col].fillna("").astype(str).str.lower().str.contains(
        re.escape(keyword), na=False, regex=True
    ).sum()
    if count > 0:
        keyword_counts_auto[keyword] = count

sorted_keywords_auto = sorted(keyword_counts_auto.items(), key=lambda x: x[1], reverse=True)
for keyword, count in sorted_keywords_auto:
    pct = count / len(df_sample) * 100
    report_lines.append(f"  '{keyword}': {count} ({pct:.1f}%)")

# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á—ë—Ç–∞
with open(OUTPUT_REPORT, 'w', encoding='utf-8') as f:
    f.write('\n'.join(report_lines))

print(f"\n[OK] EDA –æ—Ç—á—ë—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {OUTPUT_REPORT}")

# –í—ã–≤–æ–¥ –Ω–∞ —ç–∫—Ä–∞–Ω
print("\n" + "\n".join(report_lines))


