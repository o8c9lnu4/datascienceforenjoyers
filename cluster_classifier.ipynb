{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster Classifier\n",
    "\n",
    "## –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ\n",
    "\n",
    "–ó–∞–ø—É—Å—Ç–∏—Ç–µ —è—á–µ–π–∫–∏ –ø–æ –ø–æ—Ä—è–¥–∫—É."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –∑–∞–ø—Ä–æ—Å–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏\\n–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏—é (–∫–ª–∞—Å—Ç–µ—Ä) –∑–∞–ø—Ä–æ—Å–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è\\n\\n–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:\\n    python cluster_classifier.py                    # –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º\\n    python cluster_classifier.py --query \"–ø–æ–ª—É—á–∏—Ç—å –∫–æ–¥\"  # –û–¥–∏–Ω –∑–∞–ø—Ä–æ—Å\\n    python cluster_classifier.py --train            # –ü–µ—Ä–µ–æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –∑–∞–ø—Ä–æ—Å–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏\n",
    "–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏—é (–∫–ª–∞—Å—Ç–µ—Ä) –∑–∞–ø—Ä–æ—Å–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è\n",
    "\n",
    "–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:\n",
    "    python cluster_classifier.py                    # –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º\n",
    "    python cluster_classifier.py --query \"–ø–æ–ª—É—á–∏—Ç—å –∫–æ–¥\"  # –û–¥–∏–Ω –∑–∞–ø—Ä–æ—Å\n",
    "    python cluster_classifier.py --train            # –ü–µ—Ä–µ–æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∫–æ–¥–∏—Ä–æ–≤–∫–∏ –¥–ª—è Windows\n",
    "if sys.platform == 'win32':\n",
    "    try:\n",
    "        if hasattr(sys.stdout, 'reconfigure'):\n",
    "            sys.stdout.reconfigure(encoding='utf-8')\n",
    "        if hasattr(sys.stderr, 'reconfigure'):\n",
    "            sys.stderr.reconfigure(encoding='utf-8')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# –ü–æ–ø—ã—Ç–∫–∞ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å sklearn\n",
    "try:\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.cluster import KMeans\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    from scipy.sparse import csr_matrix, load_npz\n",
    "    HAS_SKLEARN = True\n",
    "except ImportError:\n",
    "    HAS_SKLEARN = False\n",
    "    print(\"[ERROR] sklearn/scipy –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã!\")\n",
    "    print(\"–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install scikit-learn scipy\")\n",
    "    sys.exit(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ù–ê–°–¢–†–û–ô–ö–ò\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "VECTORIZED_DIR = \"files/vectorized\"\n",
    "VECTORIZER_FILE = os.path.join(VECTORIZED_DIR, \"vectorizer_tfidf.pkl\")\n",
    "VECTORS_FILE = os.path.join(VECTORIZED_DIR, \"vectors_tfidf_sparse.npz\")\n",
    "METADATA_FILE = os.path.join(VECTORIZED_DIR, \"metadata_tfidf.csv\")\n",
    "CLUSTER_MODEL_FILE = os.path.join(VECTORIZED_DIR, \"cluster_model.pkl\")\n",
    "CLUSTER_INFO_FILE = os.path.join(VECTORIZED_DIR, \"cluster_info.csv\")\n",
    "\n",
    "# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏\n",
    "N_CLUSTERS = 10  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ (–º–æ–∂–Ω–æ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å)\n",
    "RANDOM_STATE = 42\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –§–£–ù–ö–¶–ò–ò\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_clean(text):\n",
    "    \"\"\"–ë–∞–∑–æ–≤–∞—è –æ—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.lower().strip()\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"–ó–∞–≥—Ä—É–∑–∫–∞ –≤–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö\"\"\"\n",
    "    print(\"\\nüìÇ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...\")\n",
    "    \n",
    "    # –ó–∞–≥—Ä—É–∑–∫–∞ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞\n",
    "    if not os.path.exists(VECTORIZER_FILE):\n",
    "        raise FileNotFoundError(f\"–í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä –Ω–µ –Ω–∞–π–¥–µ–Ω: {VECTORIZER_FILE}\")\n",
    "    \n",
    "    try:\n",
    "        vectorizer = joblib.load(VECTORIZER_FILE)\n",
    "        print(f\"   ‚úÖ –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω\")\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞: {e}\")\n",
    "    \n",
    "    # –ó–∞–≥—Ä—É–∑–∫–∞ –≤–µ–∫—Ç–æ—Ä–æ–≤\n",
    "    if not os.path.exists(VECTORS_FILE):\n",
    "        raise FileNotFoundError(f\"–í–µ–∫—Ç–æ—Ä—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã: {VECTORS_FILE}\")\n",
    "    \n",
    "    # –ó–∞–≥—Ä—É–∑–∫–∞ sparse –º–∞—Ç—Ä–∏—Ü—ã (—Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ —Ñ–æ—Ä–º–∞—Ç–µ numpy)\n",
    "    try:\n",
    "        loaded = np.load(VECTORS_FILE, allow_pickle=True)\n",
    "        vectors = csr_matrix((loaded['data'], loaded['indices'], loaded['indptr']), \n",
    "                            shape=loaded['shape'])\n",
    "        print(f\"   ‚úÖ –í–µ–∫—Ç–æ—Ä—ã –∑–∞–≥—Ä—É–∂–µ–Ω—ã: {vectors.shape}\")\n",
    "    except (KeyError, ValueError) as e:\n",
    "        # –ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∏—Ç—å –∫–∞–∫ scipy sparse —Ñ–æ—Ä–º–∞—Ç\n",
    "        try:\n",
    "            vectors = load_npz(VECTORS_FILE)\n",
    "            print(f\"   ‚úÖ –í–µ–∫—Ç–æ—Ä—ã –∑–∞–≥—Ä—É–∂–µ–Ω—ã (scipy —Ñ–æ—Ä–º–∞—Ç): {vectors.shape}\")\n",
    "        except Exception as e2:\n",
    "            raise Exception(f\"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –≤–µ–∫—Ç–æ—Ä–æ–≤: {e}, {e2}\")\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –≤–µ–∫—Ç–æ—Ä–æ–≤: {e}\")\n",
    "    \n",
    "    # –ó–∞–≥—Ä—É–∑–∫–∞ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö\n",
    "    if not os.path.exists(METADATA_FILE):\n",
    "        raise FileNotFoundError(f\"–ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã: {METADATA_FILE}\")\n",
    "    \n",
    "    try:\n",
    "        metadata = pd.read_csv(METADATA_FILE)\n",
    "        print(f\"   ‚úÖ –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã: {len(metadata)} —Å—Ç—Ä–æ–∫\")\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö: {e}\")\n",
    "    \n",
    "    return vectorizer, vectors, metadata\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cluster_model(vectors, metadata, n_clusters=None, save=True):\n",
    "    \"\"\"\n",
    "    –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    vectors : sparse matrix\n",
    "        –í–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã\n",
    "    metadata : DataFrame\n",
    "        –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å —Ç–µ–∫—Å—Ç–∞–º–∏\n",
    "    n_clusters : int, optional\n",
    "        –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: N_CLUSTERS)\n",
    "    save : bool\n",
    "        –°–æ—Ö—Ä–∞–Ω—è—Ç—å –ª–∏ –º–æ–¥–µ–ª—å\n",
    "    \"\"\"\n",
    "    if n_clusters is None:\n",
    "        n_clusters = N_CLUSTERS\n",
    "    \n",
    "    print(f\"\\nüî¢ –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ (K-Means, {n_clusters} –∫–ª–∞—Å—Ç–µ—Ä–æ–≤)...\")\n",
    "    \n",
    "    # –û–±—É—á–µ–Ω–∏–µ K-Means\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=RANDOM_STATE, n_init=10, verbose=0)\n",
    "    cluster_labels = kmeans.fit_predict(vectors)\n",
    "    \n",
    "    print(f\"   ‚úÖ –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞\")\n",
    "    \n",
    "    # –ê–Ω–∞–ª–∏–∑ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤\n",
    "    cluster_info = []\n",
    "    feature_names = None\n",
    "    \n",
    "    # –ü–æ–ø—ã—Ç–∫–∞ –ø–æ–ª—É—á–∏—Ç—å –Ω–∞–∑–≤–∞–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞\n",
    "    try:\n",
    "        # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –Ω–∞–∑–≤–∞–Ω–∏–π –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
    "        vectorizer = joblib.load(VECTORIZER_FILE)\n",
    "        feature_names = vectorizer.get_feature_names_out()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # –°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∫–ª–∞—Å—Ç–µ—Ä–∞—Ö\n",
    "    for i in range(n_clusters):\n",
    "        cluster_mask = cluster_labels == i\n",
    "        cluster_size = np.sum(cluster_mask)\n",
    "        cluster_texts = metadata[cluster_mask]['cleaned'].tolist() if 'cleaned' in metadata.columns else []\n",
    "        \n",
    "        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∫–ª–∞—Å—Ç–µ—Ä–∞\n",
    "        keywords = []\n",
    "        if feature_names is not None:\n",
    "            cluster_center = kmeans.cluster_centers_[i]\n",
    "            # –ù–∞—Ö–æ–¥–∏–º —Ç–æ–ø-5 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –Ω–∞–∏–±–æ–ª—å—à–∏–º–∏ –≤–µ—Å–∞–º–∏\n",
    "            top_indices = np.argsort(cluster_center)[-5:][::-1]\n",
    "            keywords = [feature_names[idx] for idx in top_indices]\n",
    "        \n",
    "        # –ü—Ä–∏–º–µ—Ä—ã —Ç–µ–∫—Å—Ç–æ–≤ –∏–∑ –∫–ª–∞—Å—Ç–µ—Ä–∞\n",
    "        examples = cluster_texts[:3] if cluster_texts else []\n",
    "        \n",
    "        cluster_info.append({\n",
    "            'cluster_id': i,\n",
    "            'size': cluster_size,\n",
    "            'percentage': (cluster_size / len(cluster_labels)) * 100,\n",
    "            'keywords': ', '.join(keywords) if keywords else 'N/A',\n",
    "            'examples': ' | '.join(examples[:3]) if examples else 'N/A'\n",
    "        })\n",
    "    \n",
    "    cluster_df = pd.DataFrame(cluster_info)\n",
    "    \n",
    "    if save:\n",
    "        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏\n",
    "        joblib.dump(kmeans, CLUSTER_MODEL_FILE)\n",
    "        print(f\"   üíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {CLUSTER_MODEL_FILE}\")\n",
    "        \n",
    "        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∫–ª–∞—Å—Ç–µ—Ä–∞—Ö\n",
    "        cluster_df.to_csv(CLUSTER_INFO_FILE, index=False, encoding='utf-8')\n",
    "        print(f\"   üíæ –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∫–ª–∞—Å—Ç–µ—Ä–∞—Ö —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {CLUSTER_INFO_FILE}\")\n",
    "    \n",
    "    return kmeans, cluster_df, cluster_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cluster_model():\n",
    "    \"\"\"–ó–∞–≥—Ä—É–∑–∫–∞ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏\"\"\"\n",
    "    if not os.path.exists(CLUSTER_MODEL_FILE):\n",
    "        return None, None\n",
    "    \n",
    "    try:\n",
    "        kmeans = joblib.load(CLUSTER_MODEL_FILE)\n",
    "        cluster_info = pd.read_csv(CLUSTER_INFO_FILE) if os.path.exists(CLUSTER_INFO_FILE) else None\n",
    "        return kmeans, cluster_info\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏: {e}\")\n",
    "        return None, None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_cluster(query, vectorizer, kmeans_model):\n",
    "    \"\"\"\n",
    "    –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∫–ª–∞—Å—Ç–µ—Ä–∞ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    query : str\n",
    "        –¢–µ–∫—Å—Ç –∑–∞–ø—Ä–æ—Å–∞\n",
    "    vectorizer : TfidfVectorizer\n",
    "        –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä\n",
    "    kmeans_model : KMeans\n",
    "        –û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : –†–µ–∑—É–ª—å—Ç–∞—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è\n",
    "    \"\"\"\n",
    "    # –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞\n",
    "    query_clean = basic_clean(query)\n",
    "    \n",
    "    if not query_clean:\n",
    "        return {\n",
    "            'cluster_id': None,\n",
    "            'confidence': 0.0,\n",
    "            'error': '–ü—É—Å—Ç–æ–π –∑–∞–ø—Ä–æ—Å'\n",
    "        }\n",
    "    \n",
    "    # –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –∑–∞–ø—Ä–æ—Å–∞\n",
    "    query_vector = vectorizer.transform([query_clean])\n",
    "    \n",
    "    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∫–ª–∞—Å—Ç–µ—Ä–∞\n",
    "    cluster_id = kmeans_model.predict(query_vector)[0]\n",
    "    \n",
    "    # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –¥–æ —Ü–µ–Ω—Ç—Ä–∞ –∫–ª–∞—Å—Ç–µ—Ä–∞ (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å)\n",
    "    cluster_center = kmeans_model.cluster_centers_[cluster_id]\n",
    "    query_dense = query_vector.toarray()[0]\n",
    "    \n",
    "    # –ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –¥–ª—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏\n",
    "    similarity = cosine_similarity([query_dense], [cluster_center])[0][0]\n",
    "    confidence = max(0.0, min(1.0, (similarity + 1) / 2))  # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∫ [0, 1]\n",
    "    \n",
    "    return {\n",
    "        'cluster_id': int(cluster_id),\n",
    "        'confidence': float(confidence),\n",
    "        'query_cleaned': query_clean\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_cluster_result(result, cluster_info=None):\n",
    "    \"\"\"–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –¥–ª—è –≤—ã–≤–æ–¥–∞\"\"\"\n",
    "    if result.get('error'):\n",
    "        return f\"[–û–®–ò–ë–ö–ê] {result['error']}\"\n",
    "    \n",
    "    cluster_id = result['cluster_id']\n",
    "    confidence = result['confidence']\n",
    "    \n",
    "    output = []\n",
    "    output.append(\"=\" * 60)\n",
    "    output.append(\"–†–ï–ó–£–õ–¨–¢–ê–¢ –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–ò\")\n",
    "    output.append(\"=\" * 60)\n",
    "    output.append(f\"–ó–∞–ø—Ä–æ—Å: {result.get('query_cleaned', 'N/A')}\")\n",
    "    output.append(f\"–ö–ª–∞—Å—Ç–µ—Ä: {cluster_id}\")\n",
    "    output.append(f\"–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence*100:.1f}%\")\n",
    "    \n",
    "    if cluster_info is not None and cluster_id < len(cluster_info):\n",
    "        cluster_data = cluster_info.iloc[cluster_id]\n",
    "        output.append(\"\")\n",
    "        output.append(\"–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∫–ª–∞—Å—Ç–µ—Ä–µ:\")\n",
    "        output.append(f\"  –†–∞–∑–º–µ—Ä: {int(cluster_data['size'])} –∑–∞–ø—Ä–æ—Å–æ–≤ ({cluster_data['percentage']:.1f}%)\")\n",
    "        output.append(f\"  –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: {cluster_data.get('keywords', 'N/A')}\")\n",
    "        if 'examples' in cluster_data and pd.notna(cluster_data['examples']):\n",
    "            output.append(f\"  –ü—Ä–∏–º–µ—Ä—ã: {cluster_data['examples']}\")\n",
    "    \n",
    "    output.append(\"=\" * 60)\n",
    "    \n",
    "    return '\\n'.join(output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_mode(vectorizer, kmeans_model, cluster_info=None):\n",
    "    \"\"\"–ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º —Ä–∞–±–æ—Ç—ã\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"–ö–õ–ê–°–°–ò–§–ò–ö–ê–¢–û–† –ó–ê–ü–†–û–°–û–í –ü–û –ö–õ–ê–°–¢–ï–†–ê–ú\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"–í–≤–µ–¥–∏—Ç–µ –∑–∞–ø—Ä–æ—Å –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –µ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞/–∫–∞—Ç–µ–≥–æ—Ä–∏–∏\")\n",
    "    print(\"–î–ª—è –≤—ã—Ö–æ–¥–∞ –≤–≤–µ–¥–∏—Ç–µ: 'exit', 'quit', '–≤—ã—Ö–æ–¥' –∏–ª–∏ 'q'\")\n",
    "    print(\"=\" * 60)\n",
    "    print()\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            # –í–≤–æ–¥ –∑–∞–ø—Ä–æ—Å–∞\n",
    "            query = input(\"\\n[–í–í–û–î] –í–≤–µ–¥–∏—Ç–µ –∑–∞–ø—Ä–æ—Å: \").strip()\n",
    "            \n",
    "            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –≤—ã—Ö–æ–¥\n",
    "            if query.lower() in ['exit', 'quit', '–≤—ã—Ö–æ–¥', 'q', '']:\n",
    "                print(\"\\n[INFO] –í—ã—Ö–æ–¥ –∏–∑ –ø—Ä–æ–≥—Ä–∞–º–º—ã...\")\n",
    "                break\n",
    "            \n",
    "            if not query:\n",
    "                print(\"[WARN] –ü—É—Å—Ç–æ–π –∑–∞–ø—Ä–æ—Å. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â–µ —Ä–∞–∑.\")\n",
    "                continue\n",
    "            \n",
    "            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ\n",
    "            print(\"\\n[PROCESSING] –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–∞...\")\n",
    "            result = predict_cluster(query, vectorizer, kmeans_model)\n",
    "            \n",
    "            # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞\n",
    "            print(\"\\n\" + format_cluster_result(result, cluster_info))\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n\\n[INFO] –ü—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º. –í—ã—Ö–æ–¥...\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"\\n[ERROR] –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –û–°–ù–û–í–ù–ê–Ø –ü–†–û–ì–†–ê–ú–ú–ê\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(query=None, train=False, clusters=None):\n",
    "    \"\"\"\n",
    "    –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    query : str, optional\n",
    "        –ó–∞–ø—Ä–æ—Å –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ (–µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω - –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º)\n",
    "    train : bool, optional\n",
    "        –ü–µ—Ä–µ–æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: False)\n",
    "    clusters : int, optional\n",
    "        –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: N_CLUSTERS)\n",
    "    \"\"\"\n",
    "    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º, –∑–∞–ø—É—â–µ–Ω–æ –ª–∏ –≤ Jupyter notebook\n",
    "    try:\n",
    "        from IPython import get_ipython\n",
    "        in_notebook = get_ipython() is not None\n",
    "    except:\n",
    "        in_notebook = False\n",
    "    \n",
    "    # –ï—Å–ª–∏ –Ω–µ –≤ notebook, –∏—Å–ø–æ–ª—å–∑—É–µ–º argparse\n",
    "    if not in_notebook:\n",
    "        import argparse\n",
    "        \n",
    "        parser = argparse.ArgumentParser(\n",
    "            description='–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –∑–∞–ø—Ä–æ—Å–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏',\n",
    "            formatter_class=argparse.RawDescriptionHelpFormatter,\n",
    "            epilog=\"\"\"\n",
    "–ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:\n",
    "  python cluster_classifier.py                    # –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º\n",
    "  python cluster_classifier.py --query \"–ø–æ–ª—É—á–∏—Ç—å –∫–æ–¥\"  # –û–¥–∏–Ω –∑–∞–ø—Ä–æ—Å\n",
    "  python cluster_classifier.py --train            # –ü–µ—Ä–µ–æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å\n",
    "  python cluster_classifier.py --train --clusters 15   # –û–±—É—á–∏—Ç—å —Å 15 –∫–ª–∞—Å—Ç–µ—Ä–∞–º–∏\n",
    "            \"\"\"\n",
    "        )\n",
    "        \n",
    "        parser.add_argument('--query', '-q',\n",
    "                           type=str,\n",
    "                           default=None,\n",
    "                           help='–ó–∞–ø—Ä–æ—Å –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ (–µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω - –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º)')\n",
    "        \n",
    "        parser.add_argument('--train', '-t',\n",
    "                           action='store_true',\n",
    "                           help='–ü–µ—Ä–µ–æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏')\n",
    "        \n",
    "        parser.add_argument('--clusters', '-c',\n",
    "                           type=int,\n",
    "                           default=N_CLUSTERS,\n",
    "                           help=f'–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: {N_CLUSTERS})')\n",
    "        \n",
    "        args = parser.parse_args()\n",
    "        query = args.query\n",
    "        train = args.train\n",
    "        clusters = args.clusters if args.clusters != N_CLUSTERS else None\n",
    "    \n",
    "    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–ª–∏ –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é\n",
    "    if clusters is None:\n",
    "        clusters = N_CLUSTERS\n",
    "    \n",
    "    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
    "    try:\n",
    "        vectorizer, vectors, metadata = load_data()\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –¥–∞–Ω–Ω—ã—Ö: {e}\")\n",
    "        if not in_notebook:\n",
    "            sys.exit(1)\n",
    "        return\n",
    "    \n",
    "    # –ó–∞–≥—Ä—É–∑–∫–∞ –∏–ª–∏ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏\n",
    "    kmeans_model, cluster_info = load_cluster_model()\n",
    "    \n",
    "    if train or kmeans_model is None:\n",
    "        print(\"\\n[INFO] –û–±—É—á–µ–Ω–∏–µ –Ω–æ–≤–æ–π –º–æ–¥–µ–ª–∏...\")\n",
    "        kmeans_model, cluster_info, cluster_labels = train_cluster_model(\n",
    "            vectors, metadata, n_clusters=clusters, save=True\n",
    "        )\n",
    "        print(\"\\n[INFO] –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞!\")\n",
    "    else:\n",
    "        print(\"\\n[INFO] –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏\")\n",
    "        if cluster_info is not None:\n",
    "            print(f\"   –ù–∞–π–¥–µ–Ω–æ {len(cluster_info)} –∫–ª–∞—Å—Ç–µ—Ä–æ–≤\")\n",
    "    \n",
    "    # –†–µ–∂–∏–º —Ä–∞–±–æ—Ç—ã\n",
    "    if query:\n",
    "        # –û–¥–∏–Ω–æ—á–Ω—ã–π –∑–∞–ø—Ä–æ—Å\n",
    "        print(f\"\\n[INPUT] –ó–∞–ø—Ä–æ—Å: {query}\")\n",
    "        result = predict_cluster(query, vectorizer, kmeans_model)\n",
    "        print(\"\\n\" + format_cluster_result(result, cluster_info))\n",
    "    else:\n",
    "        # –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º\n",
    "        interactive_mode(vectorizer, kmeans_model, cluster_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...\n",
      "   ‚úÖ –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω\n",
      "   ‚úÖ –í–µ–∫—Ç–æ—Ä—ã –∑–∞–≥—Ä—É–∂–µ–Ω—ã: (1000, 1234)\n",
      "   ‚úÖ –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã: 1000 —Å—Ç—Ä–æ–∫\n",
      "\n",
      "[INFO] –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏\n",
      "   –ù–∞–π–¥–µ–Ω–æ 10 –∫–ª–∞—Å—Ç–µ—Ä–æ–≤\n",
      "============================================================\n",
      "–ö–õ–ê–°–°–ò–§–ò–ö–ê–¢–û–† –ó–ê–ü–†–û–°–û–í –ü–û –ö–õ–ê–°–¢–ï–†–ê–ú\n",
      "============================================================\n",
      "–í–≤–µ–¥–∏—Ç–µ –∑–∞–ø—Ä–æ—Å –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –µ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞/–∫–∞—Ç–µ–≥–æ—Ä–∏–∏\n",
      "–î–ª—è –≤—ã—Ö–æ–¥–∞ –≤–≤–µ–¥–∏—Ç–µ: 'exit', 'quit', '–≤—ã—Ö–æ–¥' –∏–ª–∏ 'q'\n",
      "============================================================\n",
      "\n",
      "\n",
      "[PROCESSING] –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–∞...\n",
      "\n",
      "============================================================\n",
      "–†–ï–ó–£–õ–¨–¢–ê–¢ –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–ò\n",
      "============================================================\n",
      "–ó–∞–ø—Ä–æ—Å: –ø–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–¥–∞\n",
      "–ö–ª–∞—Å—Ç–µ—Ä: 5\n",
      "–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: 53.9%\n",
      "\n",
      "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∫–ª–∞—Å—Ç–µ—Ä–µ:\n",
      "  –†–∞–∑–º–µ—Ä: 445 –∑–∞–ø—Ä–æ—Å–æ–≤ (44.5%)\n",
      "  –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: –Ω–µ, –ø–æ—Å—ã–ª–∫–∏, –ø–æ—Å—ã–ª–∫—É, –∑–∞–∫–∞–∑–∞, –∫–æ–¥\n",
      "  –ü—Ä–∏–º–µ—Ä—ã: –ø–æ–ª—É—á–µ–Ω–∏–µ –∫—É–∞—Ä —Ö–æ–¥–∞ | –æ–ø–µ—Ä–∞—Ç–æ—Ä–∞ –º–æ–∂–Ω–æ —Å–≤—è–∑–∞—Ç—å—Å—è | –º–Ω–µ –ø—Ä–∏—à–ª–∞ —Å–º—Å–∫–∞ –æ—Ç –ø–æ–ª—É—á–µ–Ω–∏—è –∑–∞–∫–∞–∑–∞\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# –î–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ notebook –ø—Ä–æ—Å—Ç–æ –≤—ã–∑–æ–≤–∏—Ç–µ:\n",
    "# main()                    # –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º\n",
    "# main(query=\"–ø–æ–ª—É—á–∏—Ç—å –∫–æ–¥\")  # –û–¥–∏–Ω –∑–∞–ø—Ä–æ—Å\n",
    "# main(train=True)          # –ü–µ—Ä–µ–æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å\n",
    "# main(train=True, clusters=15)  # –û–±—É—á–∏—Ç—å —Å 15 –∫–ª–∞—Å—Ç–µ—Ä–∞–º–∏\n",
    "\n",
    "# –î–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∏–∑ –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏ (–≤ .py —Ñ–∞–π–ª–µ):\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
